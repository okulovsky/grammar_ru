{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нарушение согласования"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с поиска ошибки согласования прилагательных\n",
    "\n",
    "3 типа:\n",
    "* новЫЙ\n",
    "* хорошИЙ\n",
    "* большОЙ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Построим бандл, сбалансированный по длине предложения и по корпусу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tg.grammar_ru.common import Loc\n",
    "from tg.grammar_ru.corpus import CorpusReader, CorpusBuilder, BucketCorpusBalancer\n",
    "from tg.grammar_ru.corpus.corpus_reader import read_data\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from tg.grammar_ru.components.yandex_storage.s3_yandex_helpers import S3YandexHandler\n",
    "\n",
    "from yo_fluq_ds import Queryable, Query, fluq\n",
    "\n",
    "from typing import List, Union\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "NEW = {\n",
    "    \"ая\",\n",
    "    \"ого\",\n",
    "    \"ое\",\n",
    "    \"ой\",\n",
    "    \"ом\",\n",
    "    \"ому\",\n",
    "    \"ую\",\n",
    "    \"ые\",\n",
    "    \"ый\",\n",
    "    \"ым\",\n",
    "    \"ыми\",\n",
    "    \"ых\"\n",
    "}\n",
    "# NOTE выкинули 'ою'\n",
    "\n",
    "GOOD = {\n",
    "    \"ая\",\n",
    "    \"его\",\n",
    "    \"ее\",\n",
    "    \"ей\",\n",
    "    \"ем\",\n",
    "    \"ему\",\n",
    "    \"ие\",\n",
    "    \"ий\",\n",
    "    \"им\",\n",
    "    \"ими\",\n",
    "    \"их\",\n",
    "    \"ую\",\n",
    "    \"яя\",\n",
    "    \"юю\",\n",
    "    \"ого\",\n",
    "    \"ое\",\n",
    "    \"ой\",\n",
    "    \"ому\",\n",
    "    \"ом\",\n",
    "}  # легкий\n",
    "\n",
    "BIG = {\n",
    "    \"ая\",\n",
    "    \"ие\",\n",
    "    \"им\",\n",
    "    \"ими\",\n",
    "    \"их\",\n",
    "    \"ого\",\n",
    "    \"ое\",\n",
    "    \"ой\",\n",
    "    \"ом\",\n",
    "    \"ому\",\n",
    "    \"ую\",\n",
    "    \"ые\",\n",
    "    \"ым\",\n",
    "    \"ыми\",\n",
    "    \"ых\",\n",
    "}  # золотой\n",
    "# NOTE выкинули 'ою'\n",
    "\n",
    "NEW_list = sorted(list(NEW))\n",
    "GOOD_list = sorted(list(GOOD))\n",
    "BIG_list = sorted(list(BIG))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "index = pd.read_parquet(Loc.data_cache_path / 'bundles/agreement/adj_full/index.parquet')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tg.common.ml.batched_training import IndexedDataBundle\n",
    "from tg.common import DataBundle\n",
    "\n",
    "db = DataBundle.load(Loc.data_cache_path/'bundles/agreement/adj_full')\n",
    "idb = IndexedDataBundle(index, db)\n",
    "idb.bundle"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Отправка бандла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tg.grammar_ru.components import PlainContextBuilder\n",
    "\n",
    "context_builder = PlainContextBuilder(\n",
    "    include_zero_offset=False,\n",
    "    left_to_right_contexts_proportion=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tg.common.ml.batched_training import context as btc\n",
    "from tg.grammar_ru.components import CoreExtractor\n",
    "\n",
    "def create_assembly_point(context_length = 6):\n",
    "    ap = btc.ContextualAssemblyPoint(\n",
    "        name = 'features',\n",
    "        context_builder = context_builder,\n",
    "        extractor = CoreExtractor(join_column='another_word_id'),\n",
    "        context_length=context_length\n",
    "    )\n",
    "    ap.reduction_type = ap.reduction_type.Dim3Folded\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = create_assembly_point(context_length=15)\n",
    "ap.hidden_size = 50\n",
    "ap.dim_3_network_factory.network_type = btc.Dim3NetworkType.LSTM\n",
    "head_factory = ap.create_network_factory()\n",
    "# head = head_factory(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def _update_sizes_with_argument(argument_name, argument, sizes, modificator):\n",
    "    if argument is None:\n",
    "        return sizes\n",
    "    elif isinstance(argument, torch.Tensor):\n",
    "        return modificator(sizes, argument.shape[1])\n",
    "    elif isinstance(argument, pd.DataFrame):\n",
    "        return modificator(sizes, argument.shape[1])\n",
    "    elif isinstance(argument, int):\n",
    "        return modificator(sizes, argument)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Argument {argument_name} is supposed to be int, Tensor or none, but was `{argument}`\")\n",
    "\n",
    "\n",
    "class FullyConnectedNetwork(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 sizes: List[int],\n",
    "                 input: Union[None, torch.Tensor, int] = None,\n",
    "                 output: Union[None, torch.Tensor, int] = None):\n",
    "        super(FullyConnectedNetwork, self).__init__()\n",
    "        sizes = _update_sizes_with_argument(\n",
    "            'input', input, sizes, lambda s, v: [v] + s)\n",
    "        sizes = _update_sizes_with_argument(\n",
    "            'output', output, sizes, lambda s, v: s + [v])\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(len(sizes) - 1):\n",
    "            self.layers.append(torch.nn.Linear(sizes[i], sizes[i + 1]))\n",
    "        \n",
    "        \n",
    "    def forward(self, input):\n",
    "        X = input\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "            # X = torch.sigmoid(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tg.common.ml.batched_training import factories as btf\n",
    "\n",
    "\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(self, head, hidden_size, batch):\n",
    "        super(Network, self).__init__()\n",
    "        self.head = head\n",
    "        self.tail = FullyConnectedNetwork(\n",
    "            sizes=[], input=hidden_size, output=batch.index_frame.label.nunique())\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.tail(self.head(batch))\n",
    "\n",
    "\n",
    "class NetworkFactory:\n",
    "    def __init__(self, assembly_point):\n",
    "        self.assembly_point = assembly_point\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        head_factory = self.assembly_point.create_network_factory()\n",
    "        head = head_factory(batch)\n",
    "        return Network(head, self.assembly_point.hidden_size,  batch)\n",
    "\n",
    "\n",
    "network_factory = NetworkFactory(ap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tg.common.ml import batched_training as bt\n",
    "from sklearn.metrics import roc_auc_score, f1_score\n",
    "\n",
    "class MulticlassMetrics(bt.Metric):\n",
    "    def get_names(self):\n",
    "        return ['roc_auc', 'f1_weighted']\n",
    "\n",
    "    def measure(self, df, _):\n",
    "        prefix = 'predicted_label_'\n",
    "        start_idx = len(NEW_list) + len(GOOD_list)\n",
    "        target = (df.label - start_idx).tolist()\n",
    "        probas = np.zeros(shape=[len(df), idb.index_frame.label.nunique()])\n",
    "        for i, (_, row) in enumerate(df.iterrows()):\n",
    "            for j in range(probas.shape[1]):\n",
    "                probas[i][j] = row[f'{prefix}{start_idx + j}']\n",
    "    \n",
    "        preds = np.argmax(probas, axis=1).tolist()\n",
    "        result = []\n",
    "        result.append(roc_auc_score(target, probas, multi_class='ovo'))\n",
    "        result.append(f1_score(target, preds, average='weighted'))\n",
    "        return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from typing import *\n",
    "from tg.common.ml import batched_training as bt\n",
    "import pandas as pd\n",
    "from tg.common.ml.batched_training.factories.conventions import Conventions\n",
    "import torch\n",
    "from yo_fluq_ds import Obj\n",
    "\n",
    "\n",
    "class MulticlassPredictionInterpreter:\n",
    "    def interpret(self, input, labels, output):\n",
    "        result = input[\"index\"].copy()\n",
    "        output = torch.softmax(output, dim=1)\n",
    "        for i, c in enumerate(labels.columns):\n",
    "            result[\"true_\" + c] = labels[c]\n",
    "            result[\"predicted_\" + c] = output[:, i].tolist()\n",
    "        return result"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tg.common.ml.batched_training.factories import CtorAdapter, TorchModelHandler\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tg.common.ml import batched_training as bt\n",
    "from tg.common.ml import dft\n",
    "\n",
    "\n",
    "def get_multilabel_extractor():\n",
    "    label_extractor = (bt.PlainExtractor\n",
    "                       .build(btf.Conventions.LabelFrame)\n",
    "                       .index()\n",
    "                       .apply(take_columns=['label'],\n",
    "                              transformer=dft.DataFrameTransformerFactory.default_factory(max_values_per_category=50))\n",
    "                       )\n",
    "    return label_extractor\n",
    "\n",
    "\n",
    "class TrainingTask(btf.TorchTrainingTask):\n",
    "    def __init__(self):\n",
    "        super(TrainingTask, self).__init__()\n",
    "        self.metric_pool = bt.MetricPool().add(MulticlassMetrics())\n",
    "        self.features_ap = create_assembly_point()\n",
    "        self.settings.mini_batch_size = None\n",
    "\n",
    "    def initialize_task(self, idb):\n",
    "        self.setup_batcher(\n",
    "            idb, [ap.create_extractor(), get_multilabel_extractor()])\n",
    "        self.setup_model(network_factory, ignore_consistancy_check=True)\n",
    "\n",
    "    def setup_model(self, network_factory, ignore_consistancy_check=False):\n",
    "        self.model_handler = TorchModelHandler(\n",
    "            network_factory,\n",
    "            self.optimizer_ctor,\n",
    "            self.loss_ctor,\n",
    "            ignore_consistancy_check,\n",
    "        )\n",
    "        self.model_handler.multiclass_prediction_interpreter = MulticlassPredictionInterpreter()        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "idb.index_frame = idb.index_frame[idb.index_frame.declension_type == 2]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "task = TrainingTask()\n",
    "task.settings.epoch_count = 40\n",
    "task.optimizer_ctor = CtorAdapter(\"torch.optim:Adam\", ('params',), lr=5e-3)\n",
    "task.loss_ctor = CtorAdapter(\"torch.nn:CrossEntropyLoss\")\n",
    "result = task.run(idb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "task.settings.continue_training = True\n",
    "task.settings.epoch_count = 100\n",
    "result = task.run(idb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "joblib.dump(result['output']['model'], 'adjectives_decl_big_model.pkl')\n",
    "joblib.dump(result['output']['batcher'], 'adjectives_decl_big_batcher.pkl')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "model = joblib.load('adjectives_decl_new_model.pkl')\n",
    "batcher = joblib.load('adjectives_decl_new_batcher.pkl')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "task = TrainingTask()\n",
    "task.model_handler = model\n",
    "task.batcher = batcher "
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "pred_db = task.predict(idb)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "def score_db(db):\n",
    "    prefix = 'predicted_label_'\n",
    "    start_idx = len(NEW_list) + len(GOOD_list)\n",
    "    target = (db.label - start_idx).tolist()\n",
    "    probas = np.zeros(shape=[len(db), idb.index_frame.label.nunique()])\n",
    "    for i, (_, row) in enumerate(db.iterrows()):\n",
    "        for j in range(probas.shape[1]):\n",
    "            probas[i][j] = row[f'{prefix}{start_idx + j}']\n",
    "    \n",
    "    preds = np.argmax(probas, axis=1).tolist()\n",
    "    return precision_recall_fscore_support(target, preds, average='weighted')\n",
    "    # return accuracy_score(target, preds)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "score_db(pred_db[pred_db.split == 'test'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "def plot_confusion_matrix(db):\n",
    "    prefix = 'predicted_label_'\n",
    "    start_idx = len(NEW_list) + len(GOOD_list)\n",
    "    target = (db.label - start_idx).tolist()\n",
    "    probas = np.zeros(shape=[len(db), idb.index_frame.label.nunique()])\n",
    "    for i, (_, row) in enumerate(db.iterrows()):\n",
    "        for j in range(probas.shape[1]):\n",
    "            probas[i][j] = row[f'{prefix}{start_idx + j}']\n",
    "\n",
    "    preds = np.argmax(probas, axis=1).tolist()\n",
    "    cm = confusion_matrix(target, preds, normalize='true').round(2)\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    ConfusionMatrixDisplay(cm, display_labels=BIG_list).plot(ax=ax)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "plot_confusion_matrix(pred_db[pred_db.split == 'test'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import tg.projects.agreement.declension_extractor as de\n",
    "import importlib\n",
    "\n",
    "importlib.reload(de)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from tg.projects.agreement.declension_extractor import AdjAgreementIndexBuilder\n",
    "from tg.grammar_ru.features import SnowballFeaturizer, PyMorphyFeaturizer\n",
    "from tg.grammar_ru import Separator\n",
    "\n",
    "test_text = 'Это моя новый машина.'\n",
    "test_text_db = Separator.build_bundle(test_text, [\n",
    "    PyMorphyFeaturizer(),\n",
    "    SnowballFeaturizer(),\n",
    "    # SlovnetFeaturizer(),\n",
    "    # SyntaxTreeFeaturizer(),\n",
    "    # SyntaxStatsFeaturizer(),\n",
    "])\n",
    "test_text_db\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, roc_auc_score, accuracy_score, balanced_accuracy_score\n",
    "\n",
    "def condense_predictions(pred_df):\n",
    "    prefix = 'predicted_label_'\n",
    "    pred_columns = pred_df.columns[pred_df.columns.str.startswith(prefix)]\n",
    "    pred_labels = pred_df[pred_columns].idxmax(axis=1)\n",
    "    pred_labels = pred_labels.apply(lambda column_name: int(column_name.split('_')[-1]))\n",
    "    condensed_df = pd.DataFrame(pred_labels, columns=['label'])\n",
    "    aib = AdjAgreementIndexBuilder()\n",
    "    condensed_df['ending'] = condensed_df.label.apply(lambda x: aib.get_ending_from_index(2, x))\n",
    "    return condensed_df"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_text_suggestions(text: str):\n",
    "    text_db = Separator.build_bundle(text, [\n",
    "        PyMorphyFeaturizer(),\n",
    "        SnowballFeaturizer(),\n",
    "    ])\n",
    "    index_df = AdjAgreementIndexBuilder().build_index(text_db, 2)\n",
    "    input_idb = IndexedDataBundle(\n",
    "        index_frame=index_df,\n",
    "        bundle=text_db\n",
    "    )\n",
    "    input_idb.index_frame['label'] = -1\n",
    "    pred_df = task.predict(input_idb)\n",
    "    return condense_predictions(pred_df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_text_suggestions('Какая дом у вас построили? У нас на большая улице построили очень дорогую дом. Кажется, что это строение живая, в каком-то смысле слова! Однако он очень молодую, ему всего один год.')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "get_text_suggestions('большая человек! молодой человек!')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "    get_text_suggestions('большой человек! молодой человек!')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Результат\n",
    "\n",
    "##### Бандл\n",
    "\n",
    "* Сбалансировали по длине и по корпусу. pub & books\n",
    "* Построили фичи\n",
    "* Отобрали прилагательные с помощью pymorphy & mystem\n",
    "* Определили типы склонения и окончания\n",
    "\n",
    "\n",
    "##### Сеть\n",
    "\n",
    "* Собрали и запустили в ноутбуке\n",
    "\n",
    "##### В процессе\n",
    "\n",
    "* Доставка\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
