{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Нарушение согласования"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предыдущая попытка в задаче согласования рода - предсказывать род и сравнивать с ответом pymorphy\n",
    "\n",
    "Недостаток - большое количество ошибочных предсказаний в словах, таких как \"морской\". \n",
    "Морской порт и цвет морской волны. Род различается, написание - нет.\n",
    "\n",
    "Попробуем другой подход - предсказание окончаний."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Начнем с поиска ошибки согласования прилагательных\n",
    "\n",
    "3 типа:\n",
    "* новЫЙ\n",
    "* хорошИЙ\n",
    "* большОЙ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Построим бандл, сбалансированный по длине предложения и по корпусу."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tg.grammar_ru.common import Loc\n",
    "from tg.grammar_ru.corpus import CorpusReader, CorpusBuilder, BucketCorpusBalancer\n",
    "from tg.grammar_ru.corpus.corpus_reader import read_data\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "from tg.grammar_ru.components.yandex_storage.s3_yandex_helpers import S3YandexHandler\n",
    "\n",
    "from yo_fluq_ds import Queryable, Query, fluq\n",
    "\n",
    "from typing import List, Union\n",
    "\n",
    "import math\n",
    "import pandas as pd\n",
    "pd.set_option('display.max_rows', 500)\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv(Loc.root_path / 'environment.env')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_NAMES = [\n",
    "    \"books.base.zip\", \n",
    "    # \"pub.base.zip\"\n",
    "]\n",
    "#TODO: add smth else?\n",
    "\n",
    "CORPUS_LIST = [Loc.corpus_path/corpus_name for corpus_name in CORPUS_NAMES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/serhio/Data/1Education/grammar-spring/s2/grammar_ru/data-cache/corpus/books.base.zip')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPUS_LIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_count(corpus_path):\n",
    "    return sum([\n",
    "        len(frame.groupby(\"sentence_id\")) \n",
    "        for frame in read_data(corpus_path)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10198/3628676302.py:4: DeprecationWarning: Call to deprecated function (or staticmethod) read_data. (Use CorpusReader.read_frames_from_several_corpora)\n",
      "  for frame in read_data(corpus_path)\n",
      "100%|██████████| 2533/2533 [00:40<00:00, 61.97it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books.base.zip = 903668 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i, corpus_name in enumerate(CORPUS_NAMES):\n",
    "    print(f\"{corpus_name} = {get_sentences_count(CORPUS_LIST[i])} sentences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_BASE = math.e\n",
    "BUCKET_PATH = Loc.corpus_path/\"prepare/buckets/buckets.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2533/2533 [03:12<00:00, 13.16it/s]\n"
     ]
    }
   ],
   "source": [
    "BucketCorpusBalancer.build_buckets_frame(CORPUS_LIST, BUCKET_PATH, LOG_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>bucket_size</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bucket</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>books.base.zip/0</th>\n",
       "      <td>[2105037, 2635119, 2635122, 2930082, 2930089, ...</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>books.base.zip/1</th>\n",
       "      <td>[7, 8, 11, 15, 10711, 10730, 10731, 10745, 107...</td>\n",
       "      <td>90591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>books.base.zip/2</th>\n",
       "      <td>[1, 2, 6, 9, 12, 14, 16, 17, 18, 19, 20, 21, 2...</td>\n",
       "      <td>434207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>books.base.zip/3</th>\n",
       "      <td>[0, 3, 4, 5, 10, 13, 10712, 10716, 10718, 1072...</td>\n",
       "      <td>355499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>books.base.zip/4</th>\n",
       "      <td>[10715, 10717, 10724, 10744, 10777, 10793, 107...</td>\n",
       "      <td>23177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>books.base.zip/5</th>\n",
       "      <td>[189579, 784022, 2391783, 5783820, 5998276, 67...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          sentences  \\\n",
       "bucket                                                                \n",
       "books.base.zip/0  [2105037, 2635119, 2635122, 2930082, 2930089, ...   \n",
       "books.base.zip/1  [7, 8, 11, 15, 10711, 10730, 10731, 10745, 107...   \n",
       "books.base.zip/2  [1, 2, 6, 9, 12, 14, 16, 17, 18, 19, 20, 21, 2...   \n",
       "books.base.zip/3  [0, 3, 4, 5, 10, 13, 10712, 10716, 10718, 1072...   \n",
       "books.base.zip/4  [10715, 10717, 10724, 10744, 10777, 10793, 107...   \n",
       "books.base.zip/5  [189579, 784022, 2391783, 5783820, 5998276, 67...   \n",
       "\n",
       "                  bucket_size  \n",
       "bucket                         \n",
       "books.base.zip/0          129  \n",
       "books.base.zip/1        90591  \n",
       "books.base.zip/2       434207  \n",
       "books.base.zip/3       355499  \n",
       "books.base.zip/4        23177  \n",
       "books.base.zip/5           65  "
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet(BUCKET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NUMBERS = [1, 2, 3, 4]\n",
    "BUCKET_NUMBERS = [2]\n",
    "BUCKET_LIMIT = 60_000\n",
    "BUCKET_LIMIT = 10_000\n",
    " #Note: забили на то что в books.base.zip/4 всего 23К предложений.\n",
    "# некторый дисбаланс все-же остался"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "BucketCorpusBalancer.filter_buckets_by_bucket_numbers(BUCKET_PATH, BUCKET_NUMBERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>bucket_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>books.base.zip/2</th>\n",
       "      <td>[1, 2, 6, 9, 12, 14, 16, 17, 18, 19, 20, 21, 2...</td>\n",
       "      <td>434207</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                          sentences  \\\n",
       "books.base.zip/2  [1, 2, 6, 9, 12, 14, 16, 17, 18, 19, 20, 21, 2...   \n",
       "\n",
       "                  bucket_size  \n",
       "books.base.zip/2       434207  "
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet(BUCKET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "BALANCED_PATH = Loc.corpus_path/\"prepare/balanced/books&pub_60K_balanced.zip\"\n",
    "BALANCED_PATH = Loc.corpus_path/\"prepare/balanced/books&pub_tiny_balanced.zip\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tg.common import Logger\n",
    "def balancing() -> None:\n",
    "    balancer = BucketCorpusBalancer(\n",
    "        buckets = pd.read_parquet(BUCKET_PATH), \n",
    "        log_base = LOG_BASE,\n",
    "        bucket_limit = BUCKET_LIMIT,\n",
    "    )\n",
    "\n",
    "    CorpusBuilder.transfuse_corpus(\n",
    "        sources = CORPUS_LIST,\n",
    "        destination = BALANCED_PATH,\n",
    "        selector = balancer\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# balancing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_12481/1361944034.py:3: DeprecationWarning: Call to deprecated function (or staticmethod) read_data. (Use CorpusReader.read_frames_from_several_corpora)\n",
      "  for frame in read_data(BALANCED_PATH):\n",
      "100%|██████████| 147/147 [00:11<00:00, 12.57it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'books.base.zip': 203177, 'pub.base.zip': 240000})"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "lengths = defaultdict(int)\n",
    "for frame in read_data(BALANCED_PATH):\n",
    "    for corpus_name in CORPUS_NAMES:\n",
    "        lengths[corpus_name] += len(frame[frame.original_corpus_id == corpus_name].groupby(\"sentence_id\"))\n",
    "\n",
    "lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tg.grammar_ru import CorpusReader\n",
    "from tg.grammar_ru.corpus import CorpusBuilder\n",
    "from pathlib import Path\n",
    "\n",
    "# corpus = Path('files/corpus.zip')\n",
    "reader = CorpusReader(BALANCED_PATH)\n",
    "# reader.get_toc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dfs = reader.read_frames().take(10).to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set('ий его ему им ем ее его ему ее им ем ая ей ую ей ую ие их им ими'.split())\n",
    "# set('ый ого ому ым ом ое ая ой ую ою ые ых ыми'.split())\n",
    "# set('ой ого ому им ом ое ого ому ое им ом ая ой ую ой ою ие их им ими'.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for norm_end, group in adjs.groupby('norm_ending'):\n",
    "#     print(group[['word', 'ending', 'true_ending', 'norm_form', 'norm_ending', 'true_norm_ending']])\n",
    "#     print('####')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Построили features для сбалансированного корпуса"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tg.grammar_ru.corpus import CorpusBuilder\n",
    "from tg.grammar_ru.features import PyMorphyFeaturizer, SlovnetFeaturizer, SyntaxStatsFeaturizer, SyntaxTreeFeaturizer\n",
    "from pathlib import Path\n",
    "\n",
    "featurizers = [\n",
    "    PyMorphyFeaturizer(),\n",
    "    SlovnetFeaturizer(),\n",
    "    SyntaxTreeFeaturizer(),\n",
    "    SyntaxStatsFeaturizer()\n",
    "]\n",
    "\n",
    "CorpusBuilder.featurize_corpus(\n",
    "    Loc.data_cache_path/\"corpus/prepare/balanced\"/'books&pub_60K_balanced.zip',\n",
    "    Loc.data_cache_path/\"corpus/prepare/balanced\"/'books&pub_60K_balanced_feat.zip',\n",
    "    featurizers\n",
    ")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. О том, как строим бандл"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Существует 3 типа склонения:\n",
    " * новЫЙ\n",
    " * хорошИЙ\n",
    " * большОЙ\n",
    "\n",
    "\n",
    "Начнем с того, что будем предсказывать окончания только для одного типа склонения.\n",
    "При этом хотим построить бандл, универсальный для задачи согласования прилагательных.\n",
    "\n",
    "Отфильтровываем только прилагательные.\n",
    "\n",
    "pymorphy считает прилагательными такие слова как `один, тот, этот, сей, ваш`\n",
    "\n",
    "Поэтому отфильтровали еще и mystem'ом. Он различает числительное-прилагательное, местоимение-прилагательное.\n",
    "\n",
    "Определим тип склонения по окончанию нормальной формы, взятой из pymorphy.\n",
    "\n",
    "label'ом будет окончание прилагательного.\n",
    "\n",
    "Составили список возможных окончаний. Берем прилагательное, проверяем на какое из этих окончаний оно заканчивается.\n",
    "\n",
    "Если ни на какое - ставим Nan, выкидываем из индекса.\n",
    "\n",
    "Список прилагательных с неопределенными нами окончаниями записали в `temp/undefined_ending.txt`\n",
    "\n",
    "Кажется, туда попали только\n",
    "* притяжательные прилагательные. \"птичьи\", \"Надин\"\n",
    "* возвратные причастия. \"светящийся\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Номера окончаний"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "new = {'ая', 'ого', 'ое', 'ой', 'ом', 'ому',\n",
    "       'ою', 'ую', 'ые', 'ый', 'ым', 'ыми', 'ых'}\n",
    "\n",
    "good = {'ая', 'его', 'ее', 'ей', 'ем', 'ему',\n",
    "        'ие', 'ий', 'им', 'ими', 'их', 'ую', 'яя', 'юю'}\n",
    "\n",
    "big = {'ая', 'ие', 'им', 'ими', 'их', 'ого',\n",
    "       'ое', 'ой', 'ом', 'ому', 'ою', 'ую'}\n",
    "\n",
    "POSSIBLE_ENDINGS = set().union(new, good, big)\n",
    "endings_nums = {e: i for i, e in enumerate(\n",
    "    sorted(list(POSSIBLE_ENDINGS)))}\n",
    "\n",
    "new_declination_labels = {num for e, num in endings_nums.items() if e in new}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "25"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(POSSIBLE_ENDINGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_declination_labels"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Оставили в индексе только прилагательный 1-го типа склонения. \"Новый\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Loc' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 8\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtg\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mgrammar_ru\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcomponents\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mplain_context_builder\u001b[39;00m \u001b[39mimport\u001b[39;00m PlainContextBuilder\n\u001b[1;32m      5\u001b[0m \u001b[39m# db = DataBundle.load(Loc.data_cache_path/'bundles/agreement/mid+_mystemless')\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39m# db['index'] = db.index[db.index.label.isin(new_declination_labels)]\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[39m# db = db.copy()\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m bundle_1st_declination_path \u001b[39m=\u001b[39m Loc\u001b[39m.\u001b[39mdata_cache_path\u001b[39m/\u001b[39m\u001b[39m'\u001b[39m\u001b[39mbundles/agreement/mid+_mystemless_1st_declination\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      9\u001b[0m \u001b[39m# db.save(bundle_1st_declination_path)\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m# idb = IndexedDataBundle(db.index, db)\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Loc' is not defined"
     ]
    }
   ],
   "source": [
    "from tg.common import DataBundle\n",
    "from tg.common.ml.batched_training import IndexedDataBundle\n",
    "from tg.grammar_ru.components.plain_context_builder import PlainContextBuilder\n",
    "\n",
    "# db = DataBundle.load(Loc.data_cache_path/'bundles/agreement/mid+_mystemless')\n",
    "# db['index'] = db.index[db.index.label.isin(new_declination_labels)]\n",
    "# db = db.copy()\n",
    "bundle_1st_declination_path = Loc.data_cache_path/'bundles/agreement/mid+_mystemless_1st_declination'\n",
    "# db.save(bundle_1st_declination_path)\n",
    "# idb = IndexedDataBundle(db.index, db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'index': {'shape': (122638, 4), 'index_name': 'sample_id'}, 'pymorphy': {'shape': (2499338, 16), 'index_name': 'word_id'}, 'slovnet': {'shape': (2499338, 18), 'index_name': 'word_id'}, 'src': {'shape': (2499338, 18), 'index_name': None}, 'syntax_closure': {'shape': (5241405, 4), 'index_name': 'entry_id'}, 'syntax_fixes': {'shape': (2499338, 4), 'index_name': 'word_id'}, 'syntax_stats': {'shape': (2499338, 6), 'index_name': 'word_id'}}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Отправка бандла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "project_name = 'agreementproject'\n",
    "dataset_name = 'agreement_adj_mid+_mystemless_1st_declination'\n",
    "bucket = 'agreementadjbucket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tg.grammar_ru.components.yandex_storage.s3_yandex_helpers import S3YandexHandler\n",
    "# try:\n",
    "#     S3YandexHandler.create_bucket(bucket)\n",
    "# except:\n",
    "#     pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3path = f'datasphere/{project_name}/datasets/{dataset_name}'\n",
    "S3YandexHandler.upload_folder(bucket, s3path, bundle_1st_declination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "plain_context_builder = PlainContextBuilder(include_zero_offset=False,\n",
    "                          left_to_right_contexts_proportion=0.5)\n",
    "contexts = plain_context_builder.build_context(idb, context_size=6).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tg.grammar_ru.components import PlainContextBuilder\n",
    "\n",
    "context_builder = PlainContextBuilder(\n",
    "    include_zero_offset=True,\n",
    "    left_to_right_contexts_proportion=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tg.common.ml.batched_training import context as btc\n",
    "from tg.grammar_ru.components import CoreExtractor\n",
    "\n",
    "def create_assembly_point(context_length = 6):\n",
    "    ap = btc.ContextualAssemblyPoint(\n",
    "        name = 'features',\n",
    "        context_builder = context_builder,\n",
    "        extractor = CoreExtractor(join_column='another_word_id'),\n",
    "        context_length=context_length\n",
    "    )\n",
    "    ap.reduction_type = ap.reduction_type.Dim3Folded\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "ap = create_assembly_point()\n",
    "ap.hidden_size = 50\n",
    "ap.dim_3_network_factory.network_type = btc.Dim3NetworkType.LSTM\n",
    "head_factory = ap.create_network_factory()\n",
    "# head = head_factory(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "\n",
    "def _update_sizes_with_argument(argument_name, argument, sizes, modificator):\n",
    "    if argument is None:\n",
    "        return sizes\n",
    "    elif isinstance(argument, torch.Tensor):\n",
    "        return modificator(sizes, argument.shape[1])\n",
    "    elif isinstance(argument, pd.DataFrame):\n",
    "        return modificator(sizes, argument.shape[1])\n",
    "    elif isinstance(argument, int):\n",
    "        return modificator(sizes, argument)\n",
    "    else:\n",
    "        raise ValueError(\n",
    "            f\"Argument {argument_name} is supposed to be int, Tensor or none, but was `{argument}`\")\n",
    "\n",
    "\n",
    "class FullyConnectedNetwork(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 sizes: List[int],\n",
    "                 input: Union[None, torch.Tensor, int] = None,\n",
    "                 output: Union[None, torch.Tensor, int] = None):\n",
    "        super(FullyConnectedNetwork, self).__init__()\n",
    "        sizes = _update_sizes_with_argument(\n",
    "            'input', input, sizes, lambda s, v: [v] + s)\n",
    "        sizes = _update_sizes_with_argument(\n",
    "            'output', output, sizes, lambda s, v: s + [v])\n",
    "        self.layers = torch.nn.ModuleList()\n",
    "        for i in range(len(sizes) - 1):\n",
    "            self.layers.append(torch.nn.Linear(sizes[i], sizes[i + 1]))\n",
    "\n",
    "    def forward(self, input):\n",
    "        X = input\n",
    "        for layer in self.layers:\n",
    "            X = layer(X)\n",
    "            X = torch.sigmoid(X)\n",
    "        return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from tg.common.ml.batched_training import factories as btf\n",
    "\n",
    "\n",
    "class Network(torch.nn.Module):\n",
    "    def __init__(self, head, hidden_size, batch):\n",
    "        super(Network, self).__init__()\n",
    "        self.head = head\n",
    "        self.tail = FullyConnectedNetwork(\n",
    "            sizes=[], input=hidden_size, output=batch.index_frame.label.nunique())\n",
    "\n",
    "    def forward(self, batch):\n",
    "        return self.tail(self.head(batch))\n",
    "\n",
    "\n",
    "class NetworkFactory:\n",
    "    def __init__(self, assembly_point):\n",
    "        self.assembly_point = assembly_point\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        head_factory = self.assembly_point.create_network_factory()\n",
    "        head = head_factory(batch)\n",
    "        return Network(head, self.assembly_point.hidden_size,  batch)\n",
    "\n",
    "\n",
    "network_factory = NetworkFactory(ap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tg.common.ml import batched_training as bt\n",
    "\n",
    "# class MulticlassMetrics(bt.Metric):\n",
    "#     def __init__(self, add_accuracy=True, add_rating=False):\n",
    "#         self.add_accuracy = add_accuracy\n",
    "#         self.add_rating = add_rating\n",
    "\n",
    "#     def get_names(self):\n",
    "#         result = []\n",
    "#         if self.add_accuracy:\n",
    "#             result.append('accuracy')\n",
    "#         if self.add_rating:\n",
    "#             result.append('rating')\n",
    "#         return result\n",
    "\n",
    "#     def measure(self, df, _):\n",
    "#         prefix = 'true_label_'\n",
    "#         labels = []\n",
    "#         for c in df.columns:\n",
    "#             if c.startswith(prefix):\n",
    "#                 labels.append(c.replace(prefix, ''))\n",
    "\n",
    "#         def ustack(df, prefix, cols, name):\n",
    "#             df = df[[prefix+c for c in cols]]\n",
    "#             df.columns = [c for c in cols]\n",
    "#             df = df.unstack().to_frame(name)\n",
    "#             return df\n",
    "\n",
    "#         predicted = ustack(df, 'predicted_label_', labels, 'predicted')\n",
    "#         true = ustack(df, 'true_label_', labels, 'true')\n",
    "#         df = predicted.merge(true, left_index=True,\n",
    "#                              right_index=True).reset_index()\n",
    "#         df.columns = ['label', 'sample', 'predicted', 'true']\n",
    "#         df = df.feed(fluq.add_ordering_column(\n",
    "#             'sample', ('predicted', False), 'predicted_rating'))\n",
    "\n",
    "#         match = (df.loc[df.predicted_rating ==\n",
    "#                  0].set_index('sample').true > 0.5)\n",
    "#         rating = df.loc[df.true > 0.5].set_index('sample').predicted_rating\n",
    "#         result = []\n",
    "#         if self.add_accuracy:\n",
    "#             result.append(match.mean())\n",
    "#         if self.add_rating:\n",
    "#             result.append(rating.mean())\n",
    "#         return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import roc_auc_score\n",
    "# import pandas as pd\n",
    "# from tg.common.ml import batched_training as bt\n",
    "# from tg.common.ml import dft\n",
    "\n",
    "\n",
    "# def get_multilabel_extractor():\n",
    "#     label_extractor = (bt.PlainExtractor\n",
    "#                        .build(btf.Conventions.LabelFrame)\n",
    "#                        .index()\n",
    "#                        .apply(take_columns=['label'],\n",
    "#                               transformer=dft.DataFrameTransformerFactory.default_factory())\n",
    "#                        )\n",
    "#     return label_extractor\n",
    "\n",
    "\n",
    "# class TrainingTask(btf.TorchTrainingTask):\n",
    "#     def __init__(self):\n",
    "#         super(TrainingTask, self).__init__()\n",
    "#         self.metric_pool = bt.MetricPool().add(MulticlassMetrics())\n",
    "#         self.features_ap = create_assembly_point()\n",
    "\n",
    "#     def initialize_task(self, idb):\n",
    "#         self.setup_batcher(\n",
    "#             idb, [ap.create_extractor(), get_multilabel_extractor()])\n",
    "#         self.setup_model(network_factory, ignore_consistancy_check=True)\n",
    "\n",
    "\n",
    "# task = TrainingTask()\n",
    "# task.settings.epoch_count = 2\n",
    "# result = task.run(db)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Результат\n",
    "\n",
    "##### Бандл\n",
    "\n",
    "* Сбалансировали по длине и по корпусу. pub & books\n",
    "* Построили фичи\n",
    "* Отобрали прилагательные с помощью pymorphy & mystem\n",
    "* Определили типы склонения и окончания\n",
    "\n",
    "\n",
    "##### Сеть\n",
    "\n",
    "* Собрали и запустили в ноутбуке\n",
    "\n",
    "##### В процессе\n",
    "\n",
    "* Доставка\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
