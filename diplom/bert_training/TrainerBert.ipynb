{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T10:57:22.265572Z",
     "start_time": "2024-05-24T10:57:18.368783Z"
    }
   },
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "from grammar_ru.corpus import CorpusReader, CorpusBuilder\n",
    "from diplom.utils.corpus_utils import CorpusFramework\n",
    "from diplom.utils.dialog_markuper import DialogMarkupFeaturizer\n",
    "#from diplom.utils.speech_action_maker import SpeechActionFeaturizer\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.nn.functional import cosine_similarity\n",
    "from collections import defaultdict\n",
    "import torch\n",
    "#reported speach. "
   ],
   "execution_count": 1,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "bb82f9b2085f9c67",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T10:57:22.375590Z",
     "start_time": "2024-05-24T10:57:22.267751Z"
    }
   },
   "source": [
    "device = torch.device('cuda:0')\n",
    "path_corpus = Path(f\"../data/corpora/diplom.wow.zip\")\n",
    "corpus = CorpusReader(path_corpus)\n",
    "corpus_framework = CorpusFramework(corpus)\n",
    "authors = corpus.get_toc().author.unique()\n",
    "torch.cuda.get_device_properties(0)"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "a6d80263cdfcd037",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T10:57:22.457131Z",
     "start_time": "2024-05-24T10:57:22.377534Z"
    }
   },
   "source": [
    "text_corpus = pd.read_csv('../text_corpus.csv')\n",
    "\n",
    "labels = text_corpus['action'].unique().tolist()\n",
    "labels = [s.strip() for s in labels if s !='said']\n",
    "\n",
    "id2label={id:label for id,label in enumerate(labels)}\n",
    "\n",
    "label2id={label:id for id,label in enumerate(labels)}\n",
    "#there is deleting said words\n",
    "text_corpus = text_corpus.loc[text_corpus.action != 'said']\n",
    "\n",
    "text_corpus[\"labels\"]=text_corpus.action.map(lambda x: label2id[x.strip()])\n",
    "text_corpus = text_corpus.drop(['sample_id','action'], axis=1).rename({'speech':'text'},axis=1)\n",
    "NUM_LABELS= text_corpus.labels.nunique()\n",
    "\n",
    "labels\n"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "4ddeb83c4cfedcaf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T10:57:23.175421Z",
     "start_time": "2024-05-24T10:57:22.459112Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-cased')"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "6295990be9347711",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T10:57:24.795715Z",
     "start_time": "2024-05-24T10:57:23.177878Z"
    }
   },
   "source": [
    "import datasets\n",
    "\n",
    "dataset = datasets.Dataset.from_pandas(text_corpus)\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True)\n",
    "tokenized_datasets = dataset.map(tokenize_function, batched=True)\n",
    "dict_set = tokenized_datasets.train_test_split(test_size=0.1,train_size=0.9,seed=42)\n",
    "test_dataset = dict_set['test']\n",
    "train_val_dict = dict_set['train'].train_test_split(test_size=0.1,train_size=0.9,seed=42)\n",
    "train_dataset, val_dataset = train_val_dict['train'], train_val_dict['test']"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "a548addfcfd6c89c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T10:57:26.579547Z",
     "start_time": "2024-05-24T10:57:24.797124Z"
    }
   },
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"distilbert-base-cased\", num_labels=NUM_LABELS,id2label=id2label, label2id=label2id,dropout=0.4, attention_dropout=0.4)"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "3d495406f2a56c0f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T13:50:34.850659Z",
     "start_time": "2024-05-26T13:50:33.285671Z"
    }
   },
   "source": [
    "import evaluate\n",
    "\n",
    "def count_top_k(res, k = 5):\n",
    "    labels, pred = res.label_ids,res.predictions\n",
    "    # Get the indices of the top k predictions\n",
    "    top_k_indices = pred.argsort(axis=1)[:,::-1][:, :k]\n",
    "    matches = np.any(top_k_indices == np.expand_dims(labels, axis=1), axis=1)\n",
    "    count = np.sum(matches) / len(labels)\n",
    "\n",
    "    return count\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def eval_acc(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = metric.compute(predictions=predictions, references=labels)\n",
    "    return accuracy\n",
    "\n",
    "def compute_metrics(pred):\n",
    "    acc = eval_acc(pred)\n",
    "    mapk_2 = count_top_k(pred,k=2)\n",
    "    mapk_3 = count_top_k(pred,k=3)\n",
    "    mapk_5 = count_top_k(pred,k=5)\n",
    "    mapk_10 = count_top_k(pred,k=10)\n",
    "    return {'Acc == in_top_1': acc['accuracy'],\"in_top_2\":mapk_2,\"in_top_3\":mapk_3,\"in_top_5\":mapk_5,\"in_top_10\":mapk_10}\n"
   ],
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m/tmp/ipykernel_6585/2325172606.py\u001B[0m in \u001B[0;36m<cell line: 1>\u001B[0;34m()\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mimport\u001B[0m \u001B[0mevaluate\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mcount_top_k\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mres\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mk\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;36m5\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m     \u001B[0mlabels\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpred\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mres\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlabel_ids\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0mres\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpredictions\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m     \u001B[0;31m# Get the indices of the top k predictions\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/DataSpell_Projects/grammar_ru/env/lib/python3.10/site-packages/evaluate/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     27\u001B[0m \u001B[0;32mdel\u001B[0m \u001B[0mversion\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     28\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 29\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mevaluation_suite\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mEvaluationSuite\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     30\u001B[0m from .evaluator import (\n\u001B[1;32m     31\u001B[0m     \u001B[0mAudioClassificationEvaluator\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/DataSpell_Projects/grammar_ru/env/lib/python3.10/site-packages/evaluate/evaluation_suite/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mdatasets\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mversion\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mVersion\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 10\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mevaluator\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mevaluator\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     11\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mloading\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mevaluation_module_factory\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     12\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mlogging\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mget_logger\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/DataSpell_Projects/grammar_ru/env/lib/python3.10/site-packages/evaluate/evaluator/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     16\u001B[0m \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 17\u001B[0;31m     \u001B[0;32mfrom\u001B[0m \u001B[0mtransformers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpipelines\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mSUPPORTED_TASKS\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mSUPPORTED_PIPELINE_TASKS\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     18\u001B[0m     \u001B[0;32mfrom\u001B[0m \u001B[0mtransformers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpipelines\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mTASK_ALIASES\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     19\u001B[0m     \u001B[0;32mfrom\u001B[0m \u001B[0mtransformers\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpipelines\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mcheck_task\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mcheck_pipeline_task\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/DataSpell_Projects/grammar_ru/env/lib/python3.10/site-packages/transformers/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     24\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     25\u001B[0m \u001B[0;31m# Check the dependencies satisfy the minimal versions required.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 26\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mdependency_versions_check\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     27\u001B[0m from .utils import (\n\u001B[1;32m     28\u001B[0m     \u001B[0mOptionalDependencyNotAvailable\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/DataSpell_Projects/grammar_ru/env/lib/python3.10/site-packages/transformers/dependency_versions_check.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     15\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mdependency_versions_table\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mdeps\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 16\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0;34m.\u001B[0m\u001B[0mutils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mversions\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mrequire_version\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mrequire_version_core\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     17\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     18\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/DataSpell_Projects/grammar_ru/env/lib/python3.10/site-packages/transformers/utils/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     31\u001B[0m     \u001B[0mreplace_return_docstrings\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     32\u001B[0m )\n\u001B[0;32m---> 33\u001B[0;31m from .generic import (\n\u001B[0m\u001B[1;32m     34\u001B[0m     \u001B[0mContextManagers\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     35\u001B[0m     \u001B[0mExplicitEnum\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/DataSpell_Projects/grammar_ru/env/lib/python3.10/site-packages/transformers/utils/generic.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     29\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpackaging\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mversion\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     30\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 31\u001B[0;31m from .import_utils import (\n\u001B[0m\u001B[1;32m     32\u001B[0m     \u001B[0mget_torch_version\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     33\u001B[0m     \u001B[0mis_flax_available\u001B[0m\u001B[0;34m,\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/DataSpell_Projects/grammar_ru/env/lib/python3.10/site-packages/transformers/utils/import_utils.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m    157\u001B[0m \u001B[0;32mif\u001B[0m \u001B[0m_sklearn_available\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    158\u001B[0m     \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 159\u001B[0;31m         \u001B[0mimportlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmetadata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mversion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"scikit-learn\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    160\u001B[0m     \u001B[0;32mexcept\u001B[0m \u001B[0mimportlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmetadata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mPackageNotFoundError\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    161\u001B[0m         \u001B[0m_sklearn_available\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;32mFalse\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.10/importlib/metadata/__init__.py\u001B[0m in \u001B[0;36mversion\u001B[0;34m(distribution_name)\u001B[0m\n\u001B[1;32m    994\u001B[0m         \u001B[0;34m\"Version\"\u001B[0m \u001B[0mmetadata\u001B[0m \u001B[0mkey\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    995\u001B[0m     \"\"\"\n\u001B[0;32m--> 996\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mdistribution\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdistribution_name\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mversion\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    997\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    998\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.10/importlib/metadata/__init__.py\u001B[0m in \u001B[0;36mversion\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    625\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mversion\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    626\u001B[0m         \u001B[0;34m\"\"\"Return the 'Version' metadata for the distribution package.\"\"\"\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 627\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmetadata\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'Version'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    628\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    629\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.10/importlib/metadata/__init__.py\u001B[0m in \u001B[0;36mmetadata\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    610\u001B[0m             \u001B[0;32mor\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mread_text\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m''\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    611\u001B[0m         )\n\u001B[0;32m--> 612\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0m_adapters\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mMessage\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0memail\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmessage_from_string\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtext\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    613\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    614\u001B[0m     \u001B[0;34m@\u001B[0m\u001B[0mproperty\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m/usr/lib/python3.10/email/__init__.py\u001B[0m in \u001B[0;36mmessage_from_string\u001B[0;34m(s, *args, **kws)\u001B[0m\n\u001B[1;32m     36\u001B[0m     \"\"\"\n\u001B[1;32m     37\u001B[0m     \u001B[0;32mfrom\u001B[0m \u001B[0memail\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparser\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mParser\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 38\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mParser\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkws\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mparsestr\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     39\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     40\u001B[0m \u001B[0;32mdef\u001B[0m \u001B[0mmessage_from_bytes\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0ms\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m*\u001B[0m\u001B[0margs\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mkws\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "code",
   "id": "0a93258c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T10:57:28.206340Z",
     "start_time": "2024-05-24T10:57:28.202559Z"
    }
   },
   "source": [
    "from transformers import DataCollatorWithPadding\n",
    "\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "aa8fa9632f61a1bd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T10:57:28.507516Z",
     "start_time": "2024-05-24T10:57:28.208719Z"
    }
   },
   "source": [
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "y = text_corpus['labels'].values\n",
    "class_weights= torch.from_numpy(compute_class_weight('balanced',classes=np.unique(y),y=y)).float().to(device)\n",
    "\n",
    "class MyTrainer(Trainer):\n",
    "    def __init__(self, class_weights, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        # You pass the class weights when instantiating the Trainer\n",
    "        self.class_weights = class_weights\n",
    "\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        \"\"\"\n",
    "        How the loss is computed by Trainer. By default, all models return the loss in the first element.\n",
    "        Subclass and override for custom behavior.\n",
    "        \"\"\"\n",
    "        if self.label_smoother is not None and \"labels\" in inputs:\n",
    "            labels = inputs.pop(\"labels\")\n",
    "        else:\n",
    "            labels = None\n",
    "        outputs = model(**inputs)\n",
    "        # Save past state if it exists\n",
    "        # TODO: this needs to be fixed and made cleaner later.\n",
    "        if self.args.past_index >= 0:\n",
    "            self._past = outputs[self.args.past_index]\n",
    "\n",
    "        if labels is not None:\n",
    "            loss = self.label_smoother(outputs, labels)\n",
    "        else:\n",
    "            # We don't use .loss here since the model may return tuples instead of ModelOutput.\n",
    "\n",
    "            # Changes start here\n",
    "            # loss = outputs[\"loss\"] if isinstance(outputs, dict) else outputs[0]\n",
    "            logits = outputs['logits']\n",
    "            criterion = torch.nn.CrossEntropyLoss(weight=self.class_weights)\n",
    "            loss = criterion(logits, inputs['labels'])\n",
    "            # Changes end here\n",
    "\n",
    "        return (loss, outputs) if return_outputs else loss"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "id": "1b9837562c115761",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-24T11:00:22.196294Z",
     "start_time": "2024-05-24T10:57:37.120143Z"
    }
   },
   "source": [
    "def count_top_k(res, k = 5):\n",
    "    labels, pred = res.label_ids,res.predictions\n",
    "    # Get the indices of the top k predictions\n",
    "    top_k_indices = pred.argsort(axis=1)[:,::-1][:, :k]\n",
    "    matches = np.any(top_k_indices == np.expand_dims(labels, axis=1), axis=1)\n",
    "    count = np.sum(matches) / len(labels)\n",
    "\n",
    "    return {f'in_top_{k}': count}\n",
    "\n",
    "\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir = './LessLRTrainBert', #Выходной каталог\n",
    "    num_train_epochs = 100, #Кол-во эпох для обучения\n",
    "    per_device_train_batch_size = 8, #Размер пакета для каждого устройства во время обучения\n",
    "    per_device_eval_batch_size = 8, #Размер пакета для каждого устройства во время валидации\n",
    "    weight_decay =0.01, #Понижение весов\n",
    "    logging_dir = './sec_logs', #Каталог для хранения журналов\n",
    "    load_best_model_at_end = True, #Загружать ли лучшую модель после обучения\n",
    "    learning_rate = 1e-5, #Скорость обучения\n",
    "    evaluation_strategy ='epoch', #Валидация после каждой эпохи (можно сделать после конкретного кол-ва шагов)\n",
    "    logging_strategy = 'epoch', #Логирование после каждой эпохи\n",
    "    save_strategy = 'epoch', #Сохранение после каждой эпохи\n",
    "    save_total_limit = 1,\n",
    "    fp16=True,\n",
    "    seed=42)\n",
    "\n",
    "trainer = Trainer(tokenizer=tokenizer,\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=test_dataset,\n",
    "    compute_metrics=count_top_k,\n",
    "    data_collator=data_collator,\n",
    ")#MyTrainer(#Trainer(\n",
    "    #class_weights=class_weights,\n",
    "trainer.train()"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6acca670",
   "metadata": {},
   "source": [
    "trainer.train()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2b6b8514a796991",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-23T16:29:08.844412Z",
     "start_time": "2024-05-23T16:29:03.992803Z"
    },
    "scrolled": true
   },
   "source": [
    "trainer.train()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc83cde9",
   "metadata": {},
   "source": [
    "model_path = \"./experiments/fine-tune-bert\"\n",
    "model.save_pretrained(model_path)\n",
    "tokenizer.save_pretrained(model_path)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e70452",
   "metadata": {},
   "source": [
    "def get_prediction():\n",
    "    test_pred = trainer.predict(test_dataset)\n",
    "    labels = np.argmax(test_pred.predictions, axis = -1)\n",
    "    return labels\n",
    "pred = get_prediction()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c92f3f6",
   "metadata": {},
   "source": [
    "test_dataset.to_pandas()[['text','labels']]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6bfeff4",
   "metadata": {},
   "source": [
    "true_ans = test_dataset.to_pandas()[['text','labels']]\n",
    "true_ans.labels = [id2label[pr] for pr in true_ans.labels]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c66aaa",
   "metadata": {},
   "source": [
    "pred_l = test_dataset.to_pandas()[['text','labels']]\n",
    "pred_l.labels = [id2label[pr] for pr in pred]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca4c93e5",
   "metadata": {},
   "source": [
    "pd.merge(true_ans,pred_l,on='text',suffixes=['_true','_pred'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb7cb062",
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
