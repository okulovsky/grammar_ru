{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T21:59:27.092790Z",
     "start_time": "2024-05-26T21:59:20.376978Z"
    }
   },
   "source": [
    "from tqdm.auto import tqdm\n",
    "from grammar_ru.corpus import CorpusReader, CorpusBuilder\n",
    "from diplom.utils.corpus_utils import CorpusFramework\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.simplefilter('ignore')\n",
    "import torch\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "import numpy as np\n",
    "from diplom.bert_training.mydatasets import MyDataset\n",
    "import logging\n",
    "import pytorch_lightning as pl\n",
    "from diplom.bert_training.metrics import *\n",
    "logging.basicConfig(level=logging.ERROR)"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T21:59:27.106915Z",
     "start_time": "2024-05-26T21:59:27.094618Z"
    }
   },
   "cell_type": "code",
   "source": "pl.seed_everything(42)",
   "id": "6421e643574a2d0b",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "8266f80e4c7c78eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T21:59:27.114375Z",
     "start_time": "2024-05-26T21:59:27.108141Z"
    }
   },
   "source": [
    "device = torch.device('cuda:0')\n",
    "# path_corpus = Path(f\"../data/corpora/diplom.wow.zip\")\n",
    "# corpus = CorpusReader(path_corpus)\n",
    "# corpus_framework = CorpusFramework(corpus)\n",
    "# authors = corpus.get_toc().author.unique()\n",
    "torch.cuda.get_device_properties(0)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "_CudaDeviceProperties(name='AMD Radeon RX 6600 XT', major=10, minor=3, gcnArchName='gfx1030', total_memory=8176MB, multi_processor_count=16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "963be2b28ea68a2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T21:59:27.150887Z",
     "start_time": "2024-05-26T21:59:27.116848Z"
    }
   },
   "source": [
    "text_corpus = pd.read_csv('../filtered_updated_text_corpus.csv',index_col=0)#pd.read_csv('./text_corpus.csv')\n",
    "\n",
    "labels = text_corpus['action'].unique().tolist()\n",
    "labels = [s.strip() for s in labels if s !='said']\n",
    "\n",
    "id2label={id:label for id,label in enumerate(labels)}\n",
    "\n",
    "label2id={label:id for id,label in enumerate(labels)}\n",
    "#there is deleting said words\n",
    "text_corpus = text_corpus.loc[text_corpus.action != 'said']\n",
    "\n",
    "text_corpus[\"labels\"]=text_corpus.action.map(lambda x: label2id[x.strip()])\n",
    "text_corpus = text_corpus.drop(['action'], axis=1).rename({'speech':'text'},axis=1)#'sample_id',\n",
    "NUM_LABELS= text_corpus.labels.nunique()\n",
    "\n",
    "labels\n"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['muttered',\n",
       " 'sounded',\n",
       " 'called',\n",
       " 'exclaimed',\n",
       " 'murmured',\n",
       " 'grumbled',\n",
       " 'shouted',\n",
       " 'growled',\n",
       " 'wailed',\n",
       " 'laughed',\n",
       " 'say',\n",
       " 'intoned',\n",
       " 'spoke',\n",
       " 'snorted',\n",
       " 'sighed',\n",
       " 'mumbled',\n",
       " 'whined',\n",
       " 'squeaked',\n",
       " 'roared',\n",
       " 'retorted',\n",
       " 'barked',\n",
       " 'moaned',\n",
       " 'sound',\n",
       " 'cried',\n",
       " 'screamed',\n",
       " 'hissed',\n",
       " 'rumbled',\n",
       " 'spoken',\n",
       " 'rasped',\n",
       " 'spat',\n",
       " 'gasped',\n",
       " 'poked',\n",
       " 'bellowed',\n",
       " 'grunted',\n",
       " 'scoffed',\n",
       " 'croaked',\n",
       " 'whimpered',\n",
       " 'grown',\n",
       " 'sniffed',\n",
       " 'snarled',\n",
       " 'claimed',\n",
       " 'sobbed',\n",
       " 'squeezed',\n",
       " 'groaned',\n",
       " 'shivered',\n",
       " 'crooned',\n",
       " 'cackled',\n",
       " 'hummed',\n",
       " 'yelped',\n",
       " 'sang',\n",
       " 'grew',\n",
       " 'voiced',\n",
       " 'cooed',\n",
       " 'wept',\n",
       " 'grasped',\n",
       " 'talked',\n",
       " 'yawned',\n",
       " 'shrieked',\n",
       " 'squealed',\n",
       " 'howled',\n",
       " 'purred',\n",
       " 'giggled',\n",
       " 'yelled',\n",
       " 'screeched',\n",
       " 'rattled',\n",
       " 'chanted',\n",
       " 'grow',\n",
       " 'scolded',\n",
       " 'claim']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T21:59:28.441254Z",
     "start_time": "2024-05-26T21:59:27.152530Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from functools import partial\n",
    "import evaluate\n",
    "\n",
    "def count_top_k(pred,labels, k = 5):\n",
    "    # Get the indices of the top k predictions\n",
    "    top_k_indices = pred.argsort(axis=1)[:,::-1][:, :k]\n",
    "    matches = np.any(top_k_indices == np.expand_dims(labels, axis=1), axis=1)\n",
    "    count = np.sum(matches) / len(labels)\n",
    "\n",
    "    return count\n",
    "\n",
    "metric = evaluate.load(\"accuracy\")\n",
    "\n",
    "def eval_acc(logits, labels):\n",
    "    predictions = np.argmax(logits, axis=-1)\n",
    "    accuracy = metric.compute(predictions=predictions, references=labels)\n",
    "    return accuracy['accuracy']\n",
    "\n",
    "top_2 = partial(count_top_k,k=2)\n",
    "top_3 = partial(count_top_k,k=3)\n",
    "top_5 = partial(count_top_k,k=5)\n",
    "top_10 = partial(count_top_k,k=10)"
   ],
   "id": "d69528abbfc3f5ca",
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "id": "1516058dd968b75e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T21:59:29.020109Z",
     "start_time": "2024-05-26T21:59:28.442927Z"
    }
   },
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "MAX_LEN = 512\n",
    "TRAIN_BATCH_SIZE = 4 # 4 is totaly work fine\n",
    "VALID_BATCH_SIZE = 2\n",
    "EPOCHS = 10\n",
    "LEARNING_RATE = 1e-05#5e-04#\n",
    "tokenizer = AutoTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "\n",
    "from diplom.bert_training.configs import *\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torchmetrics import Accuracy\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "y = text_corpus['labels'].values\n",
    "class_weights= torch.from_numpy(compute_class_weight('balanced',classes=np.unique(y),y=y)).float().to(device)\n",
    "\n",
    "\n",
    "model_config = MyBertConfig(label2id=label2id,id2label=id2label,model_name=\"distilbert-base-uncased\")\n",
    "\n",
    "train_config = MyTrainConfig(NUM_LABELS=NUM_LABELS,criterion=CrossEntropyLoss(weight=class_weights),head_hidd_dim=768,out_head_dropout=0.1)\n",
    "\n",
    "optim_config = MyOptimizerConfig(\n",
    "    optimizer=torch.optim.Adam,\n",
    "    optimizer_params={'lr':LEARNING_RATE},\n",
    "    scheduler=torch.optim.lr_scheduler.StepLR,\n",
    "    scheduler_params={'step_size':5,'gamma':0.85})\n",
    "#scheduler=None,scheduler_params=None)\n",
    "#scheduler=get_linear_schedule_with_warmup,\n",
    "#scheduler_params={'num_warmup_steps':20,'num_training_steps':50})\n",
    "\n",
    "metrics_config = MyMetricsConfig(metrics=[eval_acc,top_2,top_3,top_5,top_10],names=['Acc == in_top_1',\"in_top_2\",\"in_top_3\",\"in_top_5\",\"in_top_10\"])"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "code",
   "id": "89ff3ba4f84b1454",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T21:59:29.032001Z",
     "start_time": "2024-05-26T21:59:29.021415Z"
    }
   },
   "source": [
    "train_size = 0.8\n",
    "train_dataset=text_corpus.sample(frac=train_size,random_state=200)\n",
    "test_dataset=text_corpus.drop(train_dataset.index).reset_index(drop=True)\n",
    "train_dataset = train_dataset.reset_index(drop=True)\n",
    "\n",
    "\n",
    "print(\"FULL Dataset: {}\".format(text_corpus.shape))\n",
    "print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n",
    "print(\"TEST Dataset: {}\".format(test_dataset.shape))\n",
    "\n",
    "training_set = MyDataset(train_dataset, tokenizer, MAX_LEN)\n",
    "testing_set = MyDataset(test_dataset, tokenizer, MAX_LEN)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FULL Dataset: (10510, 2)\n",
      "TRAIN Dataset: (8408, 2)\n",
      "TEST Dataset: (2102, 2)\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "id": "dd2e4e37398f8a0c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T21:59:29.048534Z",
     "start_time": "2024-05-26T21:59:29.033615Z"
    }
   },
   "source": [
    "from pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping, TQDMProgressBar\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=\"checkpoints\",\n",
    "    filename=\"best-checkpoint\",\n",
    "    save_top_k=1,\n",
    "    verbose=True,\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\"\n",
    ")\n",
    "logger = TensorBoardLogger(\"lightning_logs\", name=\"distill_bert\")\n",
    "#early_stopping_callback = EarlyStopping(monitor='val_loss', patience=2)"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "id": "9ca0e24f3d5e0a56",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T21:59:29.251496Z",
     "start_time": "2024-05-26T21:59:29.050182Z"
    }
   },
   "source": [
    "#TQDMProgressBar(refresh_rate=100)\n",
    "trainer = pl.Trainer(\n",
    "    logger=logger,\n",
    "    callbacks=[checkpoint_callback,TQDMProgressBar()],#TQDMProgressBar()\n",
    "    max_epochs=EPOCHS,\n",
    "    devices=[0],\n",
    "    #enable_progress_bar=False\n",
    ")"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "code",
   "id": "b36e11e19c838a9d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T21:59:29.257797Z",
     "start_time": "2024-05-26T21:59:29.254910Z"
    }
   },
   "source": [
    "# from diplom.bert_training.lighning_BERT import BertHudLit\n",
    "# \n",
    "# model = BertHudLit(model_params=model_config,train_params=train_config,optim_params=optim_config,metrics_params=metrics_config)"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T21:59:29.263887Z",
     "start_time": "2024-05-26T21:59:29.259548Z"
    }
   },
   "cell_type": "code",
   "source": "# model.bert",
   "id": "873c9518adc5910d",
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "id": "54153c7d4d8ef7d1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T21:59:29.269330Z",
     "start_time": "2024-05-26T21:59:29.265546Z"
    }
   },
   "source": [
    "from diplom.bert_training.mydatasets  import MyDataModule\n",
    "\n",
    "data_module = MyDataModule(train_dataset,test_dataset,tokenizer,4,MAX_LEN)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T21:59:29.275066Z",
     "start_time": "2024-05-26T21:59:29.271398Z"
    }
   },
   "cell_type": "code",
   "source": "torch.set_float32_matmul_precision('medium')",
   "id": "330653a9584b244f",
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T21:59:29.290110Z",
     "start_time": "2024-05-26T21:59:29.277235Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import transformers\n",
    "\n",
    "class SentenceClassifier(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, learning_rate=5e-5):\n",
    "        super(SentenceClassifier, self).__init__()\n",
    "\n",
    "        # Load pretrained distilbert-base-uncased configured for classification with 2 labels\n",
    "        self.model = transformers.DistilBertForSequenceClassification.from_pretrained(\n",
    "            \"distilbert-base-uncased\",\n",
    "            num_labels = NUM_LABELS,\n",
    "            output_attentions = False, # Whether the model returns attentions weights.\n",
    "            output_hidden_states = False, # Whether the model returns all hidden-states.\n",
    "        )\n",
    "        self.learning_rate = learning_rate\n",
    "\n",
    "    def training_step(self, batch, batch_no):\n",
    "        \"\"\"\n",
    "        This function overrides the pl.LightningModule class. \n",
    "        \n",
    "        When trainer.fit is called, each batch from the provided data loader is fed \n",
    "        to this function successively. \n",
    "        \"\"\"\n",
    "        ids = batch[\"ids\"]\n",
    "        masks = batch[\"mask\"]\n",
    "        labels = batch[\"targets\"]\n",
    "        outputs = self.model(ids, attention_mask=masks, labels=labels)\n",
    "        preds = torch.argmax(outputs[\"logits\"], axis=1)\n",
    "        correct = sum(preds.flatten() == labels.flatten())\n",
    "        self.log(\"train_loss\", outputs[\"loss\"], on_step=True, on_epoch=True)\n",
    "        self.log(\"train_acc\", correct/len(ids), on_step=True, on_epoch=True)\n",
    "        return outputs[\"loss\"]\n",
    "\n",
    "    def validation_step(self, batch, batch_no):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        ids = batch[\"ids\"]\n",
    "        masks = batch[\"mask\"]\n",
    "        labels = batch[\"targets\"]\n",
    "        outputs = self.model(ids, attention_mask=masks, labels=labels)\n",
    "        logits = outputs[\"logits\"]\n",
    "        log_dict = {\"val_loss\": outputs[\"loss\"]}\n",
    "        log_dict['Acc == in_top_1'] = eval_acc(logits.cpu().numpy(),labels.cpu().numpy())\n",
    "        for k,top_i in zip([2,3,5,10],[top_2,top_3,top_5,top_10]):\n",
    "            log_dict[f\"in_top_{k}\"] = top_i(logits.cpu().numpy(),labels.cpu().numpy())\n",
    "        self.log_dict(log_dict,prog_bar=True,on_step=False,on_epoch=True)\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        \"\"\"\n",
    "        This is overriding a LightningModule method that is called to return the\n",
    "        optimizer used for training.\n",
    "        \"\"\"\n",
    "        return transformers.AdamW(\n",
    "            self.model.parameters(),\n",
    "            lr = self.learning_rate,\n",
    "            eps = 1e-8\n",
    "        )"
   ],
   "id": "38e7a5639622d52e",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T21:59:29.767573Z",
     "start_time": "2024-05-26T21:59:29.292206Z"
    }
   },
   "cell_type": "code",
   "source": "model = SentenceClassifier(learning_rate=1.75e-5)",
   "id": "7dc88d59e82d1f61",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "execution_count": 15
  },
  {
   "cell_type": "code",
   "id": "c05cce2c231434bf",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T22:05:10.691713Z",
     "start_time": "2024-05-26T21:59:29.769341Z"
    }
   },
   "source": [
    "trainer.fit(model, data_module)"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name  | Type                                | Params\n",
      "--------------------------------------------------------------\n",
      "0 | model | DistilBertForSequenceClassification | 67.0 M\n",
      "--------------------------------------------------------------\n",
      "67.0 M    Trainable params\n",
      "0         Non-trainable params\n",
      "67.0 M    Total params\n",
      "268.026   Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fafd62db94814f49b08985fd7614b0e1"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4eca8991afed44ccacf4eaccd4415c46"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "Validation: |          | 0/? [00:00<?, ?it/s]"
      ],
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "07ab7dc312224086aa005fde395c5c46"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 0, global step 2102: 'val_loss' reached 2.93366 (best 2.93366), saving model to '/home/mixailkys/DataSpell_Projects/grammar_ru/diplom/bert_training/checkpoints/best-checkpoint-v12.ckpt' as top 1\n"
     ]
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "code",
   "id": "5f2ca5f13ebd95e6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-05-26T22:05:10.782359Z",
     "start_time": "2024-05-26T22:05:10.692839Z"
    }
   },
   "source": "trainer.callback_metrics",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train_loss': tensor(3.7290, device='cuda:0'),\n",
       " 'train_loss_step': tensor(3.7290, device='cuda:0'),\n",
       " 'train_acc': tensor(0., device='cuda:0'),\n",
       " 'train_acc_step': tensor(0., device='cuda:0'),\n",
       " 'val_loss': tensor(2.9337, device='cuda:0'),\n",
       " 'Acc == in_top_1': tensor(0.2479, device='cuda:0'),\n",
       " 'in_top_2': tensor(0.3949, device='cuda:0'),\n",
       " 'in_top_3': tensor(0.4853, device='cuda:0'),\n",
       " 'in_top_5': tensor(0.5942, device='cuda:0'),\n",
       " 'in_top_10': tensor(0.7303, device='cuda:0'),\n",
       " 'train_loss_epoch': tensor(3.1124, device='cuda:0'),\n",
       " 'train_acc_epoch': tensor(0.2217, device='cuda:0')}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
