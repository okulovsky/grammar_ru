{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1694500",
   "metadata": {},
   "source": [
    "### Принцип работы\n",
    "\n",
    "Для задачи классификации важно, чтобы классы были примерно одного размера. При наличии нескольких корпусов, каждый из которых содержит разное количество токенов и предложений, решить задачу балансировки размеров действительно необходимо. \n",
    "\n",
    "Балансировать корпуса нужно с ориентацией на то, какими будут сэмплы. То есть, например, неправильно балансировать по количеству токенов, если каждый сэмпл это отдельное предложение, потому что в разных корпусах длины предложений могут отличаться.\n",
    "\n",
    "Поскольку предыдущие задачи работали только с предложениями, то балансировщик будет работать на уровне предложений. Пусть есть последовательность корпусов $c_1, c_2 \\ldots c_n$, через $|c_i|$ обозначим количество предложений в корпусе. На начальном этапе очень вероятно, что $|c_1| \\neq |c_2| \\ldots \\neq |c_n|$, хочется $|c_1| \\approx |c_2| \\ldots \\approx |c_n|$ с хорошей точностью. Давайте для каждого корпуса проитерируемся по его предложениям и у каждого вычислим целый логарифм длины, тогда все предложения разобьются на бакеты \n",
    "\n",
    "$$\\large B^k_{L} = \\{ s \\space | \\space s \\in c_k \\space \\wedge \\space \\lfloor \\log (|s|) \\rceil = L\\}$$\n",
    "\n",
    "где $k \\in [1 \\ldots n]$, а разброс значений $L$ зависит от основания логарифма. Затем все бакеты собираются в один датафрейм. Понижая основание логарифма можно увеличивать количество бакетов, при единичном основании, например, все предложения просто разобьются по длине. Это может быть полезно, если при дльнейшей обработке захочется в большей степени контролировать длину предложений.\n",
    "\n",
    "<img src=\"images/balancer.png\" width=\"800\"/>\n",
    "\n",
    "После разбиения на бакеты в дело вступает балансировщик. Он принимает на вход датафрейм с бакетами и число $Q$ - верхняя граница на размер бакета. И все что он делает, это из каждого бакета рандомно выбирает $Q$ предложений, а при построении индекса фильтрует предложения, которые не попадают в выборку. Это всегда будет работать корректно, поскольку каждое предложение принадлежит какому-то корпусу, и у него можно посчитать целый логарифм длины, а корпус и логарифм вместе определяют адрес бакета."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c3050f7",
   "metadata": {},
   "source": [
    "### Пример использования\n",
    "\n",
    "Код балансировщика находится в ```tg/grammar_ru/ml/corpus/corpus_balancer.py```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fffdec0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tg.grammar_ru.common import Loc\n",
    "from tg.grammar_ru.ml.corpus import CorpusReader, CorpusBuilder, BucketCorpusBalancer\n",
    "from tg.grammar_ru.ml.corpus.corpus_reader import read_data\n",
    "\n",
    "from yo_fluq_ds import Queryable, Query, fluq\n",
    "\n",
    "from typing import List, Union\n",
    "from pathlib import Path\n",
    "\n",
    "import math\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9dbe2c",
   "metadata": {},
   "source": [
    "На первом шаге формируем список корпусов, из которых будет собираться бандл и которые нужно сбалансировать"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bff89b61",
   "metadata": {},
   "outputs": [],
   "source": [
    "CORPUS_NAMES = [\n",
    "    \"books.base.zip\", \n",
    "    \"ficbook.base.zip\"\n",
    "]\n",
    "\n",
    "CORPUS_LIST = [Loc.corpus_path/corpus_name for corpus_name in CORPUS_NAMES]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "13fd46d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[PosixPath('/home/dabdya/grammar_ru/data-cache/corpus/books.base.zip'),\n",
       " PosixPath('/home/dabdya/grammar_ru/data-cache/corpus/ficbook.base.zip')]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CORPUS_LIST"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e214654e",
   "metadata": {},
   "source": [
    "Проверим исходную сбалансированность, имеет ли смысл вообще делать что-то дальше"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2c13fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentences_count(corpus_path):\n",
    "    return sum([\n",
    "        len(frame.groupby(\"sentence_id\")) \n",
    "        for frame in read_data(corpus_path)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2107f39",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 2533/2533 [00:38<00:00, 65.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "books.base.zip = 903668 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 998/998 [00:14<00:00, 68.73it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ficbook.base.zip = 120270 sentences\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for i, corpus_name in enumerate(CORPUS_NAMES):\n",
    "    print(f\"{corpus_name} = {get_sentences_count(CORPUS_LIST[i])} sentences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cf93488",
   "metadata": {},
   "source": [
    "Видно что есть большая несбалансированность, поэтому фиксируем праметры для формирования бакетов и начинаем билдинг"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8077e602",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_BASE = math.e\n",
    "BUCKET_PATH = Loc.corpus_path/\"prepare/buckets/example.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d9012e46",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 3531/3531 [04:46<00:00, 12.31it/s]\n"
     ]
    }
   ],
   "source": [
    "BucketCorpusBalancer.build_buckets_frame(CORPUS_LIST, BUCKET_PATH, LOG_BASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "75d8baa8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>bucket_size</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>bucket</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>books.base.zip/0</th>\n",
       "      <td>[2105037, 2635119, 2635122, 2930082, 2930089, ...</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>books.base.zip/1</th>\n",
       "      <td>[7, 8, 11, 15, 10711, 10730, 10731, 10745, 107...</td>\n",
       "      <td>90591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>books.base.zip/2</th>\n",
       "      <td>[1, 2, 6, 9, 12, 14, 16, 17, 18, 19, 20, 21, 2...</td>\n",
       "      <td>434207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>books.base.zip/3</th>\n",
       "      <td>[0, 3, 4, 5, 10, 13, 10712, 10716, 10718, 1072...</td>\n",
       "      <td>355499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>books.base.zip/4</th>\n",
       "      <td>[10715, 10717, 10724, 10744, 10777, 10793, 107...</td>\n",
       "      <td>23177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>books.base.zip/5</th>\n",
       "      <td>[189579, 784022, 2391783, 5783820, 5998276, 67...</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ficbook.base.zip/0</th>\n",
       "      <td>[73552, 93817, 117139, 117186, 117216, 117370,...</td>\n",
       "      <td>449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ficbook.base.zip/1</th>\n",
       "      <td>[0, 10144, 10145, 10170, 10176, 10178, 10180, ...</td>\n",
       "      <td>13029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ficbook.base.zip/2</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>55187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ficbook.base.zip/3</th>\n",
       "      <td>[10138, 10139, 10140, 10146, 10147, 10152, 101...</td>\n",
       "      <td>47695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ficbook.base.zip/4</th>\n",
       "      <td>[10198, 21538, 21556, 21565, 93821, 93824, 938...</td>\n",
       "      <td>3896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ficbook.base.zip/5</th>\n",
       "      <td>[117304, 2108553, 3192800, 4112896, 6092616, 6...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ficbook.base.zip/6</th>\n",
       "      <td>[6200776]</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            sentences  bucket_size\n",
       "bucket                                                                            \n",
       "books.base.zip/0    [2105037, 2635119, 2635122, 2930082, 2930089, ...          129\n",
       "books.base.zip/1    [7, 8, 11, 15, 10711, 10730, 10731, 10745, 107...        90591\n",
       "books.base.zip/2    [1, 2, 6, 9, 12, 14, 16, 17, 18, 19, 20, 21, 2...       434207\n",
       "books.base.zip/3    [0, 3, 4, 5, 10, 13, 10712, 10716, 10718, 1072...       355499\n",
       "books.base.zip/4    [10715, 10717, 10724, 10744, 10777, 10793, 107...        23177\n",
       "books.base.zip/5    [189579, 784022, 2391783, 5783820, 5998276, 67...           65\n",
       "ficbook.base.zip/0  [73552, 93817, 117139, 117186, 117216, 117370,...          449\n",
       "ficbook.base.zip/1  [0, 10144, 10145, 10170, 10176, 10178, 10180, ...        13029\n",
       "ficbook.base.zip/2  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...        55187\n",
       "ficbook.base.zip/3  [10138, 10139, 10140, 10146, 10147, 10152, 101...        47695\n",
       "ficbook.base.zip/4  [10198, 21538, 21556, 21565, 93821, 93824, 938...         3896\n",
       "ficbook.base.zip/5  [117304, 2108553, 3192800, 4112896, 6092616, 6...           13\n",
       "ficbook.base.zip/6                                          [6200776]            1"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet(BUCKET_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55b717b6",
   "metadata": {},
   "source": [
    "Посмотрев на распределение бакетов, можно сделать вывод о том, какие номера бакетов ```BUCKET_NUMBERS``` следует взять и какой лимит ```BUCKET_LIMIT``` на размер бакета установить. Все зависит от соотношения размеров корпусов и распределения длин предложений в них. Недостаточно механически выставлять эти гиперпараметры, иногда это может не сработать, например, когда мало данных и один из корпусов состоит преимущественно из коротких/длинных предложений.\n",
    "\n",
    "\n",
    "Если из полученных бакетов не получается отобрать бакеты с хорошой точностью, то полезно вернуться на шаг назад и попробовать снизить основание логарифма, чтобы более гибко управлять длинами предложений.\n",
    "\n",
    "В ситуации выше можно разными способами скомбинировать бакеты. Например, если оставить номера ```[2,3,4]```, то для достаточно точной балансировки придется ставить лимит на бакет около ```4000```, но можно также пожертвовать точностью на свое усмотрение и поставить размер ```4000 + K```, где ```K``` обознает перевес в сторону одного из копусов. Или можно оставить номера ```[2,3]``` и взять лимит ```45000```. \n",
    "\n",
    "Вообщем, здесь возможны разные варианты. Если данных много то скорее всего проблем не будет, но если данных мало и они сильно не сбалансированны изначально, то лучше перепроверить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12d09a27",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUCKET_NUMBERS = [2, 3, 4]\n",
    "BUCKET_LIMIT = 4000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "34ecaa50",
   "metadata": {},
   "outputs": [],
   "source": [
    "BucketCorpusBalancer.filter_buckets_by_bucket_numbers(BUCKET_PATH, BUCKET_NUMBERS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "65ec7cdb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sentences</th>\n",
       "      <th>bucket_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>books.base.zip/2</th>\n",
       "      <td>[1, 2, 6, 9, 12, 14, 16, 17, 18, 19, 20, 21, 2...</td>\n",
       "      <td>434207</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>books.base.zip/3</th>\n",
       "      <td>[0, 3, 4, 5, 10, 13, 10712, 10716, 10718, 1072...</td>\n",
       "      <td>355499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>books.base.zip/4</th>\n",
       "      <td>[10715, 10717, 10724, 10744, 10777, 10793, 107...</td>\n",
       "      <td>23177</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ficbook.base.zip/2</th>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...</td>\n",
       "      <td>55187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ficbook.base.zip/3</th>\n",
       "      <td>[10138, 10139, 10140, 10146, 10147, 10152, 101...</td>\n",
       "      <td>47695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ficbook.base.zip/4</th>\n",
       "      <td>[10198, 21538, 21556, 21565, 93821, 93824, 938...</td>\n",
       "      <td>3896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                            sentences  bucket_size\n",
       "books.base.zip/2    [1, 2, 6, 9, 12, 14, 16, 17, 18, 19, 20, 21, 2...       434207\n",
       "books.base.zip/3    [0, 3, 4, 5, 10, 13, 10712, 10716, 10718, 1072...       355499\n",
       "books.base.zip/4    [10715, 10717, 10724, 10744, 10777, 10793, 107...        23177\n",
       "ficbook.base.zip/2  [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14...        55187\n",
       "ficbook.base.zip/3  [10138, 10139, 10140, 10146, 10147, 10152, 101...        47695\n",
       "ficbook.base.zip/4  [10198, 21538, 21556, 21565, 93821, 93824, 938...         3896"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_parquet(BUCKET_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "480a410e",
   "metadata": {},
   "outputs": [],
   "source": [
    "BALANCED_PATH = Loc.corpus_path/\"prepare/balanced/example_balanced.zip\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c0b3ce",
   "metadata": {},
   "source": [
    "```BALANCED_PATH``` указывает на место, куда нужно слить корпуса. Функция ниже не является частью балансировщика и определяется там где строится бандл и выполняются другие трансфьюзы."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0f69222d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tg.common import Logger\n",
    "# Logger.disable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da4d6e1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balancing() -> None:\n",
    "    balancer = BucketCorpusBalancer(\n",
    "        buckets = pd.read_parquet(BUCKET_PATH), \n",
    "        log_base = LOG_BASE,\n",
    "        bucket_limit = BUCKET_LIMIT,\n",
    "    )\n",
    "\n",
    "    CorpusBuilder.transfuse_corpus(\n",
    "        sources = CORPUS_LIST,\n",
    "        destination = BALANCED_PATH,\n",
    "        selector = balancer.select\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "12821b23",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "balancing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "003687f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 11/11 [00:01<00:00,  6.48it/s]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "lengths = defaultdict(int)\n",
    "for frame in read_data(BALANCED_PATH):\n",
    "    for corpus_name in CORPUS_NAMES:\n",
    "        lengths[corpus_name] += len(frame[frame.original_corpus_id == corpus_name].groupby(\"sentence_id\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "df502d01",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defaultdict(int, {'books.base.zip': 12000, 'ficbook.base.zip': 11896})"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0706c47a",
   "metadata": {},
   "source": [
    "Теперь все хорошо, и можно делать последующую обработку."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
