{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7fe0b8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from yo_fluq_ds import *\n",
    "from tg.common.ml import batched_training as bt\n",
    "from tg.grammar_ru.common import Loc, DataBundle\n",
    "from tg.common.ml.batched_training import Batcher\n",
    "from tg.grammar_ru.ml.features import PyMorphyFeaturizer\n",
    "from tg.common.ml.batched_training import IndexedDataBundle\n",
    "from tg.common.ml.batched_training import train_display_test_split\n",
    "from tg.grammar_ru.ml.components.core_extractor.extractor import CoreExtractor\n",
    "from tg.grammar_ru.ml.components.plain_context_builder import PlainContextBuilder\n",
    "from tg.grammar_ru.ml.components.contextual_binding import ContextualBinding, ContextualNetworkType"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfb7b000",
   "metadata": {},
   "source": [
    "Сначала копипаста прошлой части:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "bc38f4f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pronoun_word_id</th>\n",
       "      <th>pronoun_sentence_id</th>\n",
       "      <th>candidate_word_id</th>\n",
       "      <th>candidate_sentence_id</th>\n",
       "      <th>candidate_distance</th>\n",
       "      <th>is_match</th>\n",
       "      <th>word_distance</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sample_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>514887</td>\n",
       "      <td>514880</td>\n",
       "      <td>514884</td>\n",
       "      <td>514880</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>514887</td>\n",
       "      <td>514880</td>\n",
       "      <td>514885</td>\n",
       "      <td>514880</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1903357</td>\n",
       "      <td>1903337</td>\n",
       "      <td>1903342</td>\n",
       "      <td>1903335</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101060</th>\n",
       "      <td>22837552</td>\n",
       "      <td>22837430</td>\n",
       "      <td>22837513</td>\n",
       "      <td>22837429</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>33</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101061</th>\n",
       "      <td>22837552</td>\n",
       "      <td>22837430</td>\n",
       "      <td>22837524</td>\n",
       "      <td>22837430</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101062</th>\n",
       "      <td>22837552</td>\n",
       "      <td>22837430</td>\n",
       "      <td>22837537</td>\n",
       "      <td>22837430</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101063</th>\n",
       "      <td>22837552</td>\n",
       "      <td>22837430</td>\n",
       "      <td>22837545</td>\n",
       "      <td>22837430</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101064</th>\n",
       "      <td>22837552</td>\n",
       "      <td>22837430</td>\n",
       "      <td>22837547</td>\n",
       "      <td>22837430</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>101065 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           pronoun_word_id  pronoun_sentence_id  candidate_word_id  \\\n",
       "sample_id                                                            \n",
       "0                        7                    0                  4   \n",
       "1                        7                    0                  5   \n",
       "2                   514887               514880             514884   \n",
       "3                   514887               514880             514885   \n",
       "4                  1903357              1903337            1903342   \n",
       "...                    ...                  ...                ...   \n",
       "101060            22837552             22837430           22837513   \n",
       "101061            22837552             22837430           22837524   \n",
       "101062            22837552             22837430           22837537   \n",
       "101063            22837552             22837430           22837545   \n",
       "101064            22837552             22837430           22837547   \n",
       "\n",
       "           candidate_sentence_id  candidate_distance  is_match  word_distance  \n",
       "sample_id                                                                      \n",
       "0                              0                   1         1              3  \n",
       "1                              0                   0         0              2  \n",
       "2                         514880                   1         1              3  \n",
       "3                         514880                   0         0              2  \n",
       "4                        1903335                   3         0             12  \n",
       "...                          ...                 ...       ...            ...  \n",
       "101060                  22837429                   4         0             33  \n",
       "101061                  22837430                   3         0             25  \n",
       "101062                  22837430                   2         0             14  \n",
       "101063                  22837430                   1         0              7  \n",
       "101064                  22837430                   0         0              5  \n",
       "\n",
       "[101065 rows x 7 columns]"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db = DataBundle.load(Loc.bundles_path/'antcd/wwd')\n",
    "db.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "609742e5",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pronoun_word_id</th>\n",
       "      <th>pronoun_sentence_id</th>\n",
       "      <th>candidate_word_id</th>\n",
       "      <th>candidate_sentence_id</th>\n",
       "      <th>candidate_distance</th>\n",
       "      <th>label</th>\n",
       "      <th>word_distance</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sample_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>display</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>514887</td>\n",
       "      <td>514880</td>\n",
       "      <td>514884</td>\n",
       "      <td>514880</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>514887</td>\n",
       "      <td>514880</td>\n",
       "      <td>514885</td>\n",
       "      <td>514880</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1903357</td>\n",
       "      <td>1903337</td>\n",
       "      <td>1903342</td>\n",
       "      <td>1903335</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>2011233</td>\n",
       "      <td>2010938</td>\n",
       "      <td>2011213</td>\n",
       "      <td>2010935</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>2011233</td>\n",
       "      <td>2010938</td>\n",
       "      <td>2011223</td>\n",
       "      <td>2010937</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2011233</td>\n",
       "      <td>2010938</td>\n",
       "      <td>2011227</td>\n",
       "      <td>2010937</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>test</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2021283</td>\n",
       "      <td>2021249</td>\n",
       "      <td>2021253</td>\n",
       "      <td>2021248</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2021283</td>\n",
       "      <td>2021249</td>\n",
       "      <td>2021263</td>\n",
       "      <td>2021248</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>17</td>\n",
       "      <td>train</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "           pronoun_word_id  pronoun_sentence_id  candidate_word_id  \\\n",
       "sample_id                                                            \n",
       "0                        7                    0                  4   \n",
       "1                        7                    0                  5   \n",
       "2                   514887               514880             514884   \n",
       "3                   514887               514880             514885   \n",
       "4                  1903357              1903337            1903342   \n",
       "...                    ...                  ...                ...   \n",
       "995                2011233              2010938            2011213   \n",
       "996                2011233              2010938            2011223   \n",
       "997                2011233              2010938            2011227   \n",
       "998                2021283              2021249            2021253   \n",
       "999                2021283              2021249            2021263   \n",
       "\n",
       "           candidate_sentence_id  candidate_distance  label  word_distance  \\\n",
       "sample_id                                                                    \n",
       "0                              0                   1      1              3   \n",
       "1                              0                   0      0              2   \n",
       "2                         514880                   1      1              3   \n",
       "3                         514880                   0      0              2   \n",
       "4                        1903335                   3      0             12   \n",
       "...                          ...                 ...    ...            ...   \n",
       "995                      2010935                   2      0             14   \n",
       "996                      2010937                   1      0              7   \n",
       "997                      2010937                   0      1              4   \n",
       "998                      2021248                   3      0             25   \n",
       "999                      2021248                   2      0             17   \n",
       "\n",
       "             split  \n",
       "sample_id           \n",
       "0          display  \n",
       "1            train  \n",
       "2             test  \n",
       "3            train  \n",
       "4             test  \n",
       "...            ...  \n",
       "995          train  \n",
       "996           test  \n",
       "997           test  \n",
       "998          train  \n",
       "999          train  \n",
       "\n",
       "[1000 rows x 8 columns]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "db.index = db.index.iloc[:1000]\n",
    "db.index = db.index.rename(columns={'is_match': 'label'})\n",
    "db.index['split'] = train_display_test_split(db.index)\n",
    "db.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "8eb0e229",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tg.common.ml.batched_training.data_bundle.IndexedDataBundle at 0x254434ef910>"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "idb = IndexedDataBundle(db.index, db)\n",
    "idb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0860cb80",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tg.grammar_ru.ml.components.training_task_factory import TaskFactory, Conventions\n",
    "\n",
    "def get_binary_label_extractor():\n",
    "    label_extractor = (bt.PlainExtractor\n",
    "                       .build(Conventions.LabelFrame)\n",
    "                       .index()\n",
    "                       .apply(take_columns=['label'], transformer=None))\n",
    "    return label_extractor\n",
    "\n",
    "def get_plain_context(sentence_id_column_name, word_id_column_name):\n",
    "    pcb = PlainContextBuilder(include_zero_offset=True, left_to_right_contexts_proportion=0.5)\n",
    "    pcb.sentence_id_column_name = sentence_id_column_name\n",
    "    pcb.word_id_column_name = word_id_column_name\n",
    "    plain_context = ContextualBinding(\n",
    "        name='plain_context',\n",
    "        context_length=3,\n",
    "        network_type=ContextualNetworkType.Plain,\n",
    "        hidden_size=[30],\n",
    "        context_builder=pcb,\n",
    "        extractor=CoreExtractor(join_column='another_word_id'),\n",
    "        debug=False\n",
    "    )\n",
    "    return plain_context"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c18555",
   "metadata": {},
   "source": [
    "Для каждой головы свой батч с контекстом нужных слов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c9db791",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-06 11:59:13.078511+00:00 INFO: Fitting extractor pymorphy in CoreExtractor\n",
      "2022-12-06 11:59:13.319052+00:00 INFO: Success\n",
      "2022-12-06 11:59:13.319052+00:00 INFO: Fitting extractor slovnet_morph in CoreExtractor\n",
      "2022-12-06 11:59:13.569993+00:00 INFO: Success\n",
      "2022-12-06 11:59:13.569993+00:00 INFO: Fitting extractor slovnet_syntax in CoreExtractor\n",
      "2022-12-06 11:59:13.788421+00:00 INFO: Success\n",
      "2022-12-06 11:59:13.788421+00:00 INFO: Fitting extractor syntax_fixes in CoreExtractor\n",
      "2022-12-06 11:59:13.804048+00:00 INFO: Skipped as the corresponding frame is missing from the bundle\n",
      "2022-12-06 11:59:13.804048+00:00 INFO: Fitting extractor syntax_stats in CoreExtractor\n",
      "2022-12-06 11:59:13.804048+00:00 INFO: Skipped as the corresponding frame is missing from the bundle\n",
      "2022-12-06 11:59:15.295277+00:00 INFO: Fitting extractor pymorphy in CoreExtractor\n",
      "2022-12-06 11:59:15.534832+00:00 INFO: Success\n",
      "2022-12-06 11:59:15.534832+00:00 INFO: Fitting extractor slovnet_morph in CoreExtractor\n",
      "2022-12-06 11:59:15.769065+00:00 INFO: Success\n",
      "2022-12-06 11:59:15.769065+00:00 INFO: Fitting extractor slovnet_syntax in CoreExtractor\n",
      "2022-12-06 11:59:15.971603+00:00 INFO: Success\n",
      "2022-12-06 11:59:15.971603+00:00 INFO: Fitting extractor syntax_fixes in CoreExtractor\n",
      "2022-12-06 11:59:15.971603+00:00 INFO: Skipped as the corresponding frame is missing from the bundle\n",
      "2022-12-06 11:59:15.987261+00:00 INFO: Fitting extractor syntax_stats in CoreExtractor\n",
      "2022-12-06 11:59:15.987261+00:00 INFO: Skipped as the corresponding frame is missing from the bundle\n",
      "2022-12-06 11:59:17.501436+00:00 INFO: Fitting extractor pymorphy in CoreExtractor\n",
      "2022-12-06 11:59:17.734896+00:00 INFO: Success\n",
      "2022-12-06 11:59:17.734896+00:00 INFO: Fitting extractor slovnet_morph in CoreExtractor\n",
      "2022-12-06 11:59:17.968299+00:00 INFO: Success\n",
      "2022-12-06 11:59:17.968299+00:00 INFO: Fitting extractor slovnet_syntax in CoreExtractor\n",
      "2022-12-06 11:59:18.171215+00:00 INFO: Success\n",
      "2022-12-06 11:59:18.186841+00:00 INFO: Fitting extractor syntax_fixes in CoreExtractor\n",
      "2022-12-06 11:59:18.186841+00:00 INFO: Skipped as the corresponding frame is missing from the bundle\n",
      "2022-12-06 11:59:18.186841+00:00 INFO: Fitting extractor syntax_stats in CoreExtractor\n",
      "2022-12-06 11:59:18.186841+00:00 INFO: Skipped as the corresponding frame is missing from the bundle\n",
      "2022-12-06 11:59:20.638697+00:00 INFO: Fitting extractor pymorphy in CoreExtractor\n",
      "2022-12-06 11:59:20.867925+00:00 INFO: Success\n",
      "2022-12-06 11:59:20.867925+00:00 INFO: Fitting extractor slovnet_morph in CoreExtractor\n",
      "2022-12-06 11:59:21.099572+00:00 INFO: Success\n",
      "2022-12-06 11:59:21.099572+00:00 INFO: Fitting extractor slovnet_syntax in CoreExtractor\n",
      "2022-12-06 11:59:21.319952+00:00 INFO: Success\n",
      "2022-12-06 11:59:21.335579+00:00 INFO: Fitting extractor syntax_fixes in CoreExtractor\n",
      "2022-12-06 11:59:21.335579+00:00 INFO: Skipped as the corresponding frame is missing from the bundle\n",
      "2022-12-06 11:59:21.335579+00:00 INFO: Fitting extractor syntax_stats in CoreExtractor\n",
      "2022-12-06 11:59:21.335579+00:00 INFO: Skipped as the corresponding frame is missing from the bundle\n"
     ]
    }
   ],
   "source": [
    "pronoun_pc = get_plain_context('pronoun_sentence_id', 'pronoun_word_id')\n",
    "pronoun_head_factory = pronoun_pc.create_network_factory(task=None, input=None)\n",
    "candidate_pc = get_plain_context('candidate_sentence_id', 'candidate_word_id')\n",
    "candidate_head_factory = candidate_pc.create_network_factory(task=None, input=None)\n",
    "\n",
    "context_extractors = [pronoun_pc.create_extractor(task=None, bundle=idb),\n",
    "                      candidate_pc.create_extractor(task=None, bundle=idb)]\n",
    "for extractor in context_extractors:\n",
    "    extractor.fit(idb)\n",
    "batches = [Batcher(batch_size=1000, extractors=[extractor, get_binary_label_extractor()]).fit_extract(idb) for extractor in context_extractors]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06fc0ca6",
   "metadata": {},
   "source": [
    "Попытка соединить головы:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8c309e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tg.common.ml.batched_training.torch.networks.simple_networks import FullyConnectedNetwork\n",
    "from tg.common.ml.batched_training.torch.networks.network_commons import TorchNetworkFactory\n",
    "from tg.common.ml.batched_training.torch.networks.extracting_network import FeedForwardNetwork\n",
    "\n",
    "class ParallelNetworkDifferentInputs(torch.nn.Module):\n",
    "    def __init__(self, *networks: torch.nn.Module):\n",
    "        super(ParallelNetworkDifferentInputs, self).__init__()\n",
    "        self.networks = networks[0]\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        result = []\n",
    "        for i in range(len(self.networks)):\n",
    "            network = self.networks[i]\n",
    "            result.append(network(inputs[i]))\n",
    "        return result\n",
    "\n",
    "class HeadsUnionNetwork(torch.nn.Module):\n",
    "    def __init__(self,\n",
    "                 sizes: List[int],\n",
    "                 heads,\n",
    "                 outs,\n",
    "                 output: Union[None, torch.Tensor, int] = None):\n",
    "        super(HeadsUnionNetwork, self).__init__()\n",
    "        heads_forward_networks = []\n",
    "        for i in range(len(heads)):\n",
    "            fully_connected = FullyConnectedNetwork(sizes=[3], input=outs[i].shape[1], output=output)\n",
    "            heads_forward_networks.append(FeedForwardNetwork(heads[i], fully_connected, torch.nn.Softmax(dim=1)))\n",
    "        #self.parallel_network = ParallelNetworkDifferentInputs(heads_forward_networks)\n",
    "        self.nn_layers = torch.nn.ModuleList([ParallelNetworkDifferentInputs(heads_forward_networks)])\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        for i, layer in enumerate(self.nn_layers):\n",
    "            heads_outputs = layer(inputs) #[x['plain_context'] for x in inputs]\n",
    "        concat = torch.cat([torch.transpose(x, 0, 1) for x in heads_outputs], dim=0)\n",
    "        parallel_mean = torch.mean(concat, 0)\n",
    "        return parallel_mean.view(len(parallel_mean), 1)\n",
    "    \n",
    "\n",
    "class MultiheadNetworkFactory:\n",
    "    def __init__(self, *head_factories):\n",
    "        self.head_factories = [*head_factories]\n",
    "\n",
    "    def create_network(self, task, inputs):\n",
    "        if len(self.head_factories) != len(inputs):\n",
    "            raise Error\n",
    "        heads = []\n",
    "        outs = []\n",
    "        for i in range(len(inputs)):\n",
    "            head = self.head_factories[i].create_network(task=None, input=inputs[i])\n",
    "            heads.append(head)\n",
    "            outs.append(head(inputs[i]))\n",
    "\n",
    "        return HeadsUnionNetwork(sizes=[3], heads=heads, outs=outs, output=inputs[0]['label'].shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "119761a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "network_factory = MultiheadNetworkFactory(pronoun_head_factory, candidate_head_factory)\n",
    "assembled_network = network_factory.create_network(task=None, inputs=batches)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abc30de",
   "metadata": {},
   "source": [
    "Теряются параметры"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "5dc60343",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "def train_loop(model, loss_fn, batches):\n",
    "    pred = model(batches)\n",
    "    loss = loss_fn(pred, torch.tensor(batches[0]['label'].values, dtype=torch.float32))\n",
    "\n",
    "    #optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    #optimizer.step()\n",
    "    \n",
    "learning_rate = 1\n",
    "#optimizer = torch.optim.SGD(assembled_network.parameters(), lr=learning_rate)\n",
    "epochs = 10\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(assembled_network, torch.nn.CrossEntropyLoss(), batches)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "b8371b8f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "optimizer got an empty parameter list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [94]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptim\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSGD\u001b[49m\u001b[43m(\u001b[49m\u001b[43massembled_network\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlearning_rate\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\alexandra\\desktop\\grammar_ru\\venv\\lib\\site-packages\\torch\\optim\\sgd.py:105\u001b[0m, in \u001b[0;36mSGD.__init__\u001b[1;34m(self, params, lr, momentum, dampening, weight_decay, nesterov, maximize, foreach)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m nesterov \u001b[38;5;129;01mand\u001b[39;00m (momentum \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m dampening \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[0;32m    104\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNesterov momentum requires a momentum and zero dampening\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 105\u001b[0m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSGD\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdefaults\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\alexandra\\desktop\\grammar_ru\\venv\\lib\\site-packages\\torch\\optim\\optimizer.py:49\u001b[0m, in \u001b[0;36mOptimizer.__init__\u001b[1;34m(self, params, defaults)\u001b[0m\n\u001b[0;32m     47\u001b[0m param_groups \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(params)\n\u001b[0;32m     48\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(param_groups) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 49\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moptimizer got an empty parameter list\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     50\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_groups[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m     51\u001b[0m     param_groups \u001b[38;5;241m=\u001b[39m [{\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mparams\u001b[39m\u001b[38;5;124m'\u001b[39m: param_groups}]\n",
      "\u001b[1;31mValueError\u001b[0m: optimizer got an empty parameter list"
     ]
    }
   ],
   "source": [
    "optimizer = torch.optim.SGD(assembled_network.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933df21e",
   "metadata": {},
   "source": [
    "Выглядит плохо"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6b884a",
   "metadata": {},
   "source": [
    "Смотрю на tasks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "eb5033fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tg.common.ml.batched_training import torch as btt\n",
    "from tg.common.ml.batched_training import TrainingSettings\n",
    "from tg.common.ml import batched_training as bt\n",
    "    \n",
    "class MultiheadExtractorsFactory(btt.TorchExtractorFactory):\n",
    "        def __init__(self, plain_contexts):\n",
    "            self.plain_contexts = plain_contexts\n",
    "\n",
    "        def create_extractors(self, task, bundle) -> List[bt.Extractor]:\n",
    "            context_extractors = []\n",
    "            for pc in self.plain_contexts:\n",
    "                context_extractors.append(pc.create_extractor(task=task, bundle=bundle))\n",
    "            return context_extractors\n",
    "\n",
    "class MyTrainingTask(btt.TorchTrainingTask):\n",
    "    def __init__(self,\n",
    "                 training_settings: TrainingSettings,\n",
    "                 torch_settings: btt.TorchTrainingSettings,\n",
    "                 metric_pool: bt.MetricPool,\n",
    "                 basis_tasks_sources: Optional[Dict[str, btt.AbstractBasisTaskSource]] = None\n",
    "                 ):\n",
    "        plain_contexts = [get_plain_context('pronoun_sentence_id', 'pronoun_word_id'),\n",
    "                          get_plain_context('candidate_sentence_id', 'candidate_word_id')]\n",
    "        head_factories = [pc.create_network_factory(task=None, input=None) for pc in plain_contexts]\n",
    "        \n",
    "        extractor_factory = MultiheadExtractorsFactory(plain_contexts)\n",
    "        network_factory = MultiheadNetworkFactory(*head_factories)\n",
    "        super(MyTrainingTask, self).__init__(\n",
    "            training_settings,\n",
    "            torch_settings,\n",
    "            extractor_factory,\n",
    "            network_factory,\n",
    "            metric_pool,\n",
    "            basis_tasks_sources\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "69f2b3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import typing as tp\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from tg.grammar_ru.ml.components import GrammarMirrorSettings\n",
    "from tg.common.ml import batched_training as bt\n",
    "\n",
    "\n",
    "\n",
    "features = {\n",
    "    'P': 'pymorphy',\n",
    "    'M': 'slovnet_morph',\n",
    "    'S': 'slovnet_syntax',\n",
    "    'F': 'syntax_fixes',\n",
    "    'T': 'syntax_stats'\n",
    "}\n",
    "\n",
    "\n",
    "def build_task(\n",
    "        epoch_count: int = 50,\n",
    "        batch_size: int = 20000,\n",
    "        mini_batch_size: int = 200,\n",
    "        mini_epoch_count: int = 4,\n",
    "        learning_rate: float = 0.1,\n",
    "        plain_context_length: int = 10,\n",
    "        plain_context_left_shift: float = 0.5,\n",
    "        plain_net_size: tp.List[int] = [20],\n",
    "        plain_network_mode: btm.ContextualNetworkType = btm.ContextualNetworkType.Plain,\n",
    "        plain_context_reverse: bool = False,\n",
    "        feature_allow_list: tp.Optional[tp.List[tp.Any]] = None\n",
    "        ) -> btm.MirrorTrainingTask:\n",
    "    train_settings = bt.TrainingSettings(\n",
    "        epoch_count=epoch_count,\n",
    "        batch_size=batch_size,\n",
    "        mini_batch_size=mini_batch_size,\n",
    "        mini_epoch_count=mini_epoch_count\n",
    "    )\n",
    "    torch_settings = btt.TorchTrainingSettings(\n",
    "        btt.OptimizerConstructor('torch.optim:SGD', lr=learning_rate)\n",
    "    )\n",
    "    mirror_settings = GrammarMirrorSettings()\n",
    "    mirror_settings.plain_context.context_builder.left_to_right_contexts_proportion = plain_context_left_shift\n",
    "    mirror_settings.plain_context.context_length = plain_context_length\n",
    "    mirror_settings.plain_context.hidden_size = plain_net_size\n",
    "    mirror_settings.plain_context.network_type = plain_network_mode\n",
    "    mirror_settings.plain_context.reverse_order_in_lstm = plain_context_reverse\n",
    "\n",
    "    if feature_allow_list is not None:\n",
    "        mirror_settings.plain_context.extractor.allow_list = [features[_] for _ in feature_allow_list]\n",
    "\n",
    "    task = MyTrainingTask(\n",
    "        train_settings,\n",
    "        torch_settings,\n",
    "        bt.MetricPool().add_sklearn(roc_auc_score),\n",
    "    )\n",
    "    task.info['name'] = 'TSAG-'\n",
    "    return task\n",
    "\n",
    "\n",
    "def run_local(bundle: DataBundle) -> None:\n",
    "    task = build_task(plain_network_mode=btm.ContextualNetworkType.Plain)\n",
    "    task.settings.epoch_count = 10\n",
    "    task.settings.batch_size = 1000\n",
    "    task.settings.training_batch_limit = 10\n",
    "    task.settings.evaluation_batch_limit = None\n",
    "\n",
    "    return (task, task.run(bundle))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "1c845575",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-12-06 18:39:25.849362+00:00 INFO: Training starts. Info: {'name': 'TSAG-'}\n",
      "2022-12-06 18:39:25.851361+00:00 INFO: Ensuring/loading bundle. Bundle before:\n",
      "<tg.common.ml.batched_training.data_bundle.IndexedDataBundle object at 0x00000254434EF910>\n",
      "2022-12-06 18:39:25.853362+00:00 INFO: Bundle loaded\n",
      "{'index': {'shape': (101065, 7), 'index_name': 'sample_id', 'columns': ['pronoun_word_id', 'pronoun_sentence_id', 'candidate_word_id', 'candidate_sentence_id', 'candidate_distance', '...'], 'index': [0, 1, 2, 3, 4, '...']}, 'pymorphy': {'shape': (570787, 16), 'index_name': 'word_id', 'columns': ['normal_form', 'alternatives', 'score', 'delta_score', 'POS', '...'], 'index': [0, 1, 2, 3, 4, '...']}, 'slovnet': {'shape': (570787, 17), 'index_name': 'word_id', 'columns': ['POS', 'Case', 'Animacy', 'Gender', 'Number', '...'], 'index': [0, 1, 2, 3, 4, '...']}, 'src': {'shape': (570787, 14), 'index_name': None, 'columns': ['word_id', 'sentence_id', 'word_index', 'paragraph_id', 'word_tail', '...'], 'index': [0, 1, 2, 3, 4, '...']}}\n",
      "2022-12-06 18:39:25.855363+00:00 INFO: Index frame is set to index, shape is (1000, 8)\n",
      "2022-12-06 18:39:25.857363+00:00 INFO: Running late initizalization\n",
      "2022-12-06 18:39:25.859364+00:00 INFO: No basis tasks are available\n",
      "2022-12-06 18:39:25.862365+00:00 INFO: Preprocessing bundle\n",
      "2022-12-06 18:39:25.864363+00:00 INFO: Creating extractors\n",
      "2022-12-06 18:39:25.867367+00:00 INFO: Extractors: plain_context, plain_context\n",
      "2022-12-06 18:39:25.869368+00:00 INFO: Setting batcher\n",
      "2022-12-06 18:39:25.871366+00:00 INFO: Preprocessing bundle by batcher\n",
      "2022-12-06 18:39:25.884373+00:00 INFO: Splits: train 700, test 300, display 300\n",
      "2022-12-06 18:39:25.886369+00:00 INFO: New training. Instantiating the system\n",
      "2022-12-06 18:39:25.889375+00:00 INFO: Fitting the transformers\n",
      "2022-12-06 18:39:25.972395+00:00 INFO: Fitting extractor pymorphy in CoreExtractor\n",
      "2022-12-06 18:39:26.204786+00:00 INFO: Success\n",
      "2022-12-06 18:39:26.220410+00:00 INFO: Fitting extractor slovnet_morph in CoreExtractor\n",
      "2022-12-06 18:39:26.448111+00:00 INFO: Success\n",
      "2022-12-06 18:39:26.448111+00:00 INFO: Fitting extractor slovnet_syntax in CoreExtractor\n",
      "2022-12-06 18:39:26.751061+00:00 INFO: Success\n",
      "2022-12-06 18:39:26.759066+00:00 INFO: Fitting extractor syntax_fixes in CoreExtractor\n",
      "2022-12-06 18:39:26.767481+00:00 INFO: Skipped as the corresponding frame is missing from the bundle\n",
      "2022-12-06 18:39:26.773481+00:00 INFO: Fitting extractor syntax_stats in CoreExtractor\n",
      "2022-12-06 18:39:26.785506+00:00 INFO: Skipped as the corresponding frame is missing from the bundle\n",
      "2022-12-06 18:39:28.948238+00:00 INFO: Fitting extractor pymorphy in CoreExtractor\n"
     ]
    },
    {
     "ename": "MemoryError",
     "evalue": "cannot allocate memory for array",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Input \u001b[1;32mIn [99]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mrun_local\u001b[49m\u001b[43m(\u001b[49m\u001b[43midb\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[1;32mIn [92]\u001b[0m, in \u001b[0;36mrun_local\u001b[1;34m(bundle)\u001b[0m\n\u001b[0;32m     64\u001b[0m task\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mtraining_batch_limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m\n\u001b[0;32m     65\u001b[0m task\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mevaluation_batch_limit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (task, \u001b[43mtask\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbundle\u001b[49m\u001b[43m)\u001b[49m)\n",
      "File \u001b[1;32mc:\\users\\alexandra\\desktop\\grammar_ru\\tg\\common\\ml\\training_core\\arch.py:78\u001b[0m, in \u001b[0;36mAbstractTrainingTask.run\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m     76\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun\u001b[39m(\u001b[38;5;28mself\u001b[39m, data: Any):\n\u001b[0;32m     77\u001b[0m     env \u001b[38;5;241m=\u001b[39m InMemoryTrainingEnvironment()\n\u001b[1;32m---> 78\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_with_environment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env\u001b[38;5;241m.\u001b[39mresult\n",
      "File \u001b[1;32mc:\\users\\alexandra\\desktop\\grammar_ru\\tg\\common\\ml\\batched_training\\training_task.py:354\u001b[0m, in \u001b[0;36mBatchedTrainingTask.run_with_environment\u001b[1;34m(self, _bundle, env)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrun_with_environment\u001b[39m(\u001b[38;5;28mself\u001b[39m, _bundle: Union[\u001b[38;5;28mstr\u001b[39m, Path, DataBundle], env: Optional[TrainingEnvironment] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 354\u001b[0m     temp_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_prepare_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_bundle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43menv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    355\u001b[0m     Logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInitialization completed\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    357\u001b[0m     temp_data\u001b[38;5;241m.\u001b[39mtrain_bundle \u001b[38;5;241m=\u001b[39m temp_data\u001b[38;5;241m.\u001b[39moriginal_ibundle\u001b[38;5;241m.\u001b[39mchange_index(temp_data\u001b[38;5;241m.\u001b[39msplit\u001b[38;5;241m.\u001b[39mtrain)\n",
      "File \u001b[1;32mc:\\users\\alexandra\\desktop\\grammar_ru\\tg\\common\\ml\\batched_training\\training_task.py:177\u001b[0m, in \u001b[0;36mBatchedTrainingTask._prepare_all\u001b[1;34m(self, bundle, env)\u001b[0m\n\u001b[0;32m    175\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msettings\u001b[38;5;241m.\u001b[39mcontinue_training:\n\u001b[0;32m    176\u001b[0m     Logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mNew training. Instantiating the system\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 177\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_instantiate_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43mibundle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchange_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    178\u001b[0m     first_iteration \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\users\\alexandra\\desktop\\grammar_ru\\tg\\common\\ml\\batched_training\\training_task.py:122\u001b[0m, in \u001b[0;36mBatchedTrainingTask._instantiate_all\u001b[1;34m(self, ibundle)\u001b[0m\n\u001b[0;32m    119\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhistory \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m    121\u001b[0m Logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFitting the transformers\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 122\u001b[0m test_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_extract\u001b[49m\u001b[43m(\u001b[49m\u001b[43mibundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    124\u001b[0m Logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInstantiating model\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_handler\u001b[38;5;241m.\u001b[39minstantiate(\u001b[38;5;28mself\u001b[39m, test_batch)\n",
      "File \u001b[1;32mc:\\users\\alexandra\\desktop\\grammar_ru\\tg\\common\\ml\\batched_training\\batcher.py:41\u001b[0m, in \u001b[0;36mBatcher.fit_extract\u001b[1;34m(self, ibundle)\u001b[0m\n\u001b[0;32m     39\u001b[0m ibundle \u001b[38;5;241m=\u001b[39m ibundle\u001b[38;5;241m.\u001b[39mchange_index(index_df)\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m extractor \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextractors:\n\u001b[1;32m---> 41\u001b[0m     \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mibundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     42\u001b[0m batch \u001b[38;5;241m=\u001b[39m Extractor\u001b[38;5;241m.\u001b[39mmake_extraction(ibundle, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextractors)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "File \u001b[1;32mc:\\users\\alexandra\\desktop\\grammar_ru\\tg\\common\\ml\\batched_training\\context\\architecture.py:111\u001b[0m, in \u001b[0;36mContextExtractor.fit\u001b[1;34m(self, ibundle)\u001b[0m\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_builder\u001b[38;5;241m.\u001b[39mfit(ibundle)\n\u001b[0;32m    110\u001b[0m fit_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcontext_index_frame\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_builder\u001b[38;5;241m.\u001b[39mbuild_context(ibundle, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_size)\n\u001b[1;32m--> 111\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextractors_and_aggregators \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeature_extractor_factory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_extractors_and_aggregators\u001b[49m\u001b[43m(\u001b[49m\u001b[43mibundle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchange_index\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfit_data\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcontext_index_frame\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    112\u001b[0m data \u001b[38;5;241m=\u001b[39m ExtractorInnerData(ibundle, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontext_size)\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdebug:\n",
      "File \u001b[1;32mc:\\users\\alexandra\\desktop\\grammar_ru\\tg\\common\\ml\\batched_training\\context\\architecture.py:64\u001b[0m, in \u001b[0;36mSimpleExtractorToAggregatorFactory.create_extractors_and_aggregators\u001b[1;34m(self, ibundle)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcreate_extractors_and_aggregators\u001b[39m(\u001b[38;5;28mself\u001b[39m, ibundle: IndexedDataBundle) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m List[ExtractorToAggregator]:\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mibundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     features_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mextractor\u001b[38;5;241m.\u001b[39mextract(ibundle)\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maggregators:\n",
      "File \u001b[1;32mc:\\users\\alexandra\\desktop\\grammar_ru\\tg\\grammar_ru\\ml\\components\\core_extractor\\extractor.py:50\u001b[0m, in \u001b[0;36mCoreExtractor.fit\u001b[1;34m(self, ibundle)\u001b[0m\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[0;32m     49\u001b[0m extractor\u001b[38;5;241m.\u001b[39mdisabled \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m---> 50\u001b[0m \u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextractor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mibundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     51\u001b[0m Logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSuccess\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\alexandra\\desktop\\grammar_ru\\tg\\common\\ml\\batched_training\\plain_extractor.py:134\u001b[0m, in \u001b[0;36mPlainExtractor.fit\u001b[1;34m(self, ibundle)\u001b[0m\n\u001b[0;32m    132\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, ibundle: IndexedDataBundle):\n\u001b[0;32m    133\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 134\u001b[0m         frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_build_frame\u001b[49m\u001b[43m(\u001b[49m\u001b[43mibundle\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    135\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer\u001b[38;5;241m.\u001b[39mfit(frame)\n",
      "File \u001b[1;32mc:\\users\\alexandra\\desktop\\grammar_ru\\tg\\common\\ml\\batched_training\\plain_extractor.py:119\u001b[0m, in \u001b[0;36mPlainExtractor._build_frame\u001b[1;34m(self, ibundle)\u001b[0m\n\u001b[0;32m    116\u001b[0m     how \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mleft\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    118\u001b[0m join_columns \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoins[join_index \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mkeep_columns\n\u001b[1;32m--> 119\u001b[0m current \u001b[38;5;241m=\u001b[39m \u001b[43mcurrent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mjoin_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdrop(join_columns, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m    121\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m current\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m<\u001b[39m ibundle\u001b[38;5;241m.\u001b[39mindex_frame\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n\u001b[0;32m    122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mError in extractor \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: when merging with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mjoin\u001b[38;5;241m.\u001b[39mframe\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, less rows are produced, \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcurrent\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m instead \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mibundle\u001b[38;5;241m.\u001b[39mindex_frame\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Are some rows missing?\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\users\\alexandra\\desktop\\grammar_ru\\venv\\lib\\site-packages\\pandas\\core\\frame.py:9351\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[1;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m   9332\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   9333\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[0;32m   9334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   9347\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m   9348\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   9349\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[1;32m-> 9351\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   9352\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9354\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9355\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9359\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9360\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9361\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9362\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9363\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9364\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   9365\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\alexandra\\desktop\\grammar_ru\\venv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:122\u001b[0m, in \u001b[0;36mmerge\u001b[1;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[0;32m     90\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     91\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    105\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    106\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m    107\u001b[0m     op \u001b[38;5;241m=\u001b[39m _MergeOperation(\n\u001b[0;32m    108\u001b[0m         left,\n\u001b[0;32m    109\u001b[0m         right,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    120\u001b[0m         validate\u001b[38;5;241m=\u001b[39mvalidate,\n\u001b[0;32m    121\u001b[0m     )\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\alexandra\\desktop\\grammar_ru\\venv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:716\u001b[0m, in \u001b[0;36m_MergeOperation.get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    713\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindicator:\n\u001b[0;32m    714\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indicator_pre_merge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright)\n\u001b[1;32m--> 716\u001b[0m join_index, left_indexer, right_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_join_info\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    718\u001b[0m llabels, rlabels \u001b[38;5;241m=\u001b[39m _items_overlap_with_suffix(\n\u001b[0;32m    719\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft\u001b[38;5;241m.\u001b[39m_info_axis, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright\u001b[38;5;241m.\u001b[39m_info_axis, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuffixes\n\u001b[0;32m    720\u001b[0m )\n\u001b[0;32m    722\u001b[0m lindexers \u001b[38;5;241m=\u001b[39m {\u001b[38;5;241m1\u001b[39m: left_indexer} \u001b[38;5;28;01mif\u001b[39;00m left_indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m {}\n",
      "File \u001b[1;32mc:\\users\\alexandra\\desktop\\grammar_ru\\venv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:967\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_info\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    963\u001b[0m     join_index, right_indexer, left_indexer \u001b[38;5;241m=\u001b[39m _left_join_on_index(\n\u001b[0;32m    964\u001b[0m         right_ax, left_ax, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys, sort\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msort\n\u001b[0;32m    965\u001b[0m     )\n\u001b[0;32m    966\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 967\u001b[0m     (left_indexer, right_indexer) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_join_indexers\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    969\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_index:\n\u001b[0;32m    970\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n",
      "File \u001b[1;32mc:\\users\\alexandra\\desktop\\grammar_ru\\venv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:941\u001b[0m, in \u001b[0;36m_MergeOperation._get_join_indexers\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    939\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_join_indexers\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mtuple\u001b[39m[npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp], npt\u001b[38;5;241m.\u001b[39mNDArray[np\u001b[38;5;241m.\u001b[39mintp]]:\n\u001b[0;32m    940\u001b[0m     \u001b[38;5;124;03m\"\"\"return the join indexers\"\"\"\u001b[39;00m\n\u001b[1;32m--> 941\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_join_indexers\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    942\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mleft_join_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mright_join_keys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhow\u001b[49m\n\u001b[0;32m    943\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\users\\alexandra\\desktop\\grammar_ru\\venv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:1498\u001b[0m, in \u001b[0;36mget_join_indexers\u001b[1;34m(left_keys, right_keys, sort, how, **kwargs)\u001b[0m\n\u001b[0;32m   1492\u001b[0m lkey, rkey \u001b[38;5;241m=\u001b[39m _get_join_keys(llab, rlab, shape, sort)\n\u001b[0;32m   1494\u001b[0m \u001b[38;5;66;03m# factorize keys to a dense i8 space\u001b[39;00m\n\u001b[0;32m   1495\u001b[0m \u001b[38;5;66;03m# `count` is the num. of unique keys\u001b[39;00m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# set(lkey) | set(rkey) == range(count)\u001b[39;00m\n\u001b[1;32m-> 1498\u001b[0m lkey, rkey, count \u001b[38;5;241m=\u001b[39m \u001b[43m_factorize_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;66;03m# preserve left frame order if how == 'left' and sort == False\u001b[39;00m\n\u001b[0;32m   1500\u001b[0m kwargs \u001b[38;5;241m=\u001b[39m copy\u001b[38;5;241m.\u001b[39mcopy(kwargs)\n",
      "File \u001b[1;32mc:\\users\\alexandra\\desktop\\grammar_ru\\venv\\lib\\site-packages\\pandas\\core\\reshape\\merge.py:2191\u001b[0m, in \u001b[0;36m_factorize_keys\u001b[1;34m(lk, rk, sort, how)\u001b[0m\n\u001b[0;32m   2187\u001b[0m llab \u001b[38;5;241m=\u001b[39m rizer\u001b[38;5;241m.\u001b[39mfactorize(lk)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   2188\u001b[0m \u001b[38;5;66;03m# Argument 1 to \"factorize\" of \"ObjectFactorizer\" has incompatible type\u001b[39;00m\n\u001b[0;32m   2189\u001b[0m \u001b[38;5;66;03m# \"Union[ndarray[Any, dtype[signedinteger[_64Bit]]],\u001b[39;00m\n\u001b[0;32m   2190\u001b[0m \u001b[38;5;66;03m# ndarray[Any, dtype[object_]]]\"; expected \"ndarray[Any, dtype[object_]]\"\u001b[39;00m\n\u001b[1;32m-> 2191\u001b[0m rlab \u001b[38;5;241m=\u001b[39m \u001b[43mrizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfactorize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrk\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m   2192\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m llab\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(np\u001b[38;5;241m.\u001b[39mintp), llab\u001b[38;5;241m.\u001b[39mdtype\n\u001b[0;32m   2193\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m rlab\u001b[38;5;241m.\u001b[39mdtype \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39mdtype(np\u001b[38;5;241m.\u001b[39mintp), rlab\u001b[38;5;241m.\u001b[39mdtype\n",
      "File \u001b[1;32mc:\\users\\alexandra\\desktop\\grammar_ru\\venv\\lib\\site-packages\\pandas\\_libs\\hashtable.pyx:169\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64Factorizer.factorize\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2384\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable.get_labels\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:2305\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64HashTable._unique\u001b[1;34m()\u001b[0m\n",
      "File \u001b[1;32mpandas\\_libs\\hashtable_class_helper.pxi:657\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.Int64Vector.resize\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: cannot allocate memory for array"
     ]
    }
   ],
   "source": [
    "run_local(idb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4150b916",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
