{
 "metadata": {
  "kernelspec": {
   "language": "python",
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30498,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook",
   "isGpuEnabled": true
  }
 },
 "nbformat_minor": 4,
 "nbformat": 4,
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "## Deep Learning Ulaanbaatar (DLUB) 2023 - Summer School üá≤üá≥\n",
    "\n",
    "**Seminar:** Techniques to train LLM - LoRA (low-rank approximation technique) on Mongolian GPT2 (part 2)\n",
    "\n",
    "- Pretrained model: [mongolian-gpt2](https://huggingface.co/bayartsogt/mongolian-gpt2)\n",
    "    - Huggingface Space: https://huggingface.co/spaces/flax-community/Mongolian-GPT2\n",
    "- Load dataset using [datasets](https://github.com/huggingface/datasets) library\n",
    "    - Source: [Home page of DLUB](https://sites.google.com/view/dlub/2023)\n",
    "    - Dataset: [bayartsogt/test_dlub_2023](https://huggingface.co/datasets/bayartsogt/test_dlub_2023)\n",
    "- train model using [transformers.Trainer](https://huggingface.co/docs/transformers/main_classes/trainer)\n",
    "\n",
    "\n",
    "**Claimer!**\n",
    "- Only shows for a fine-tuning pipeline demonstration purpose.\n",
    "- Contents generated from models might offend some people, but it is all/our responsibility to fix those biases. \n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Kaggle Notebook –ë—ç–ª—Ç–≥—ç–ª"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install --upgrade accelerate\n",
    "!pip install peft"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-17T21:16:20.766287Z",
     "iopub.execute_input": "2024-01-17T21:16:20.766735Z",
     "iopub.status.idle": "2024-01-17T21:16:43.045917Z",
     "shell.execute_reply.started": "2024-01-17T21:16:20.766702Z",
     "shell.execute_reply": "2024-01-17T21:16:43.044925Z"
    },
    "trusted": true
   },
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "text": "Requirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.26.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from accelerate) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.4.1)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.0.0)\nRequirement already satisfied: huggingface-hub in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.20.2)\nRequirement already satisfied: safetensors>=0.3.1 in /opt/conda/lib/python3.10/site-packages (from accelerate) (0.3.1)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->accelerate) (3.0.9)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.12.0)\nRequirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (4.5.0)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2023.5.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (2.28.2)\nRequirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub->accelerate) (4.64.1)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub->accelerate) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0mRequirement already satisfied: peft in /opt/conda/lib/python3.10/site-packages (0.7.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from peft) (1.23.5)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from peft) (21.3)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from peft) (5.9.3)\nRequirement already satisfied: pyyaml in /opt/conda/lib/python3.10/site-packages (from peft) (5.4.1)\nRequirement already satisfied: torch>=1.13.0 in /opt/conda/lib/python3.10/site-packages (from peft) (2.0.0)\nRequirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (from peft) (4.29.2)\nRequirement already satisfied: tqdm in /opt/conda/lib/python3.10/site-packages (from peft) (4.64.1)\nRequirement already satisfied: accelerate>=0.21.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.26.1)\nRequirement already satisfied: safetensors in /opt/conda/lib/python3.10/site-packages (from peft) (0.3.1)\nRequirement already satisfied: huggingface-hub>=0.17.0 in /opt/conda/lib/python3.10/site-packages (from peft) (0.20.2)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (3.12.0)\nRequirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2023.5.0)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (2.28.2)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.17.0->peft) (4.5.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->peft) (3.0.9)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (1.12)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.13.0->peft) (3.1.2)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (2023.5.5)\nRequirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /opt/conda/lib/python3.10/site-packages (from transformers->peft) (0.13.3)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.13.0->peft) (2.1.2)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2.1.1)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (3.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (1.26.15)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.17.0->peft) (2023.5.7)\nRequirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.13.0->peft) (1.3.0)\n\u001B[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001B[0m\u001B[33m\n\u001B[0m",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### [Important] Run -> Restart & clear cell outputs"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "# we upgraded `accelerate` just because to import Trainer API\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from peft import LoraConfig, get_peft_model"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-17T21:16:43.048075Z",
     "iopub.execute_input": "2024-01-17T21:16:43.048378Z",
     "iopub.status.idle": "2024-01-17T21:16:43.053492Z",
     "shell.execute_reply.started": "2024-01-17T21:16:43.048350Z",
     "shell.execute_reply": "2024-01-17T21:16:43.052511Z"
    },
    "trusted": true
   },
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## ”®–≥”©–≥–¥–ª”©”© —É–Ω—à–∏—Ö\n",
    "\n",
    "–≠–Ω—ç—Ö“Ø“Ø demo-–¥ –∑–æ—Ä–∏—É–ª–∂ –∂–∏–∂–∏–≥ —Ö—ç–º–∂—ç—ç—Ç—ç–π –¥–∞—Ç–∞ –±—ç–ª–¥—Å—ç–Ω –±–∞–π–≥–∞–∞.\n",
    "\n",
    "Data link: https://huggingface.co/datasets/bayartsogt/test_dlub_2023"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "datasets = load_dataset('bayartsogt/test_dlub_2023')"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-17T21:16:43.054733Z",
     "iopub.execute_input": "2024-01-17T21:16:43.055066Z",
     "iopub.status.idle": "2024-01-17T21:16:43.680512Z",
     "shell.execute_reply.started": "2024-01-17T21:16:43.055037Z",
     "shell.execute_reply": "2024-01-17T21:16:43.679626Z"
    },
    "trusted": true
   },
   "execution_count": 27,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/2 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "1d5b9ef5dba34feb9451b2d9b0ad2e14"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "datasets"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-17T21:16:43.682775Z",
     "iopub.execute_input": "2024-01-17T21:16:43.683399Z",
     "iopub.status.idle": "2024-01-17T21:16:43.689993Z",
     "shell.execute_reply.started": "2024-01-17T21:16:43.683364Z",
     "shell.execute_reply": "2024-01-17T21:16:43.689105Z"
    },
    "trusted": true
   },
   "execution_count": 28,
   "outputs": [
    {
     "execution_count": 28,
     "output_type": "execute_result",
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['question', 'answer'],\n        num_rows: 38\n    })\n    test: Dataset({\n        features: ['question', 'answer'],\n        num_rows: 38\n    })\n})"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def preprocess(example):\n",
    "    example[\"text\"] = (example[\"question\"] + \" \" + example[\"answer\"])\n",
    "    return example\n",
    "\n",
    "datasets = datasets.map(preprocess, remove_columns=[\"question\", \"answer\"])"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-17T21:16:43.691048Z",
     "iopub.execute_input": "2024-01-17T21:16:43.691327Z",
     "iopub.status.idle": "2024-01-17T21:16:43.742999Z",
     "shell.execute_reply.started": "2024-01-17T21:16:43.691303Z",
     "shell.execute_reply": "2024-01-17T21:16:43.742181Z"
    },
    "trusted": true
   },
   "execution_count": 29,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/38 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ca4ad3710f984f6c9e32ca36f7f50918"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/38 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "88422866a32146a998e40e15f0567f33"
      }
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "datasets"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Tokenize"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "MODEL_NAME = \"bayartsogt/mongolian-gpt2\""
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-17T21:16:46.581359Z",
     "iopub.execute_input": "2024-01-17T21:16:46.581760Z",
     "iopub.status.idle": "2024-01-17T21:16:46.586360Z",
     "shell.execute_reply.started": "2024-01-17T21:16:46.581732Z",
     "shell.execute_reply": "2024-01-17T21:16:46.585515Z"
    },
    "trusted": true
   },
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n",
    "tokenizer.pad_token = \"<pad>\""
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-17T21:16:49.422944Z",
     "iopub.execute_input": "2024-01-17T21:16:49.423671Z",
     "iopub.status.idle": "2024-01-17T21:16:49.616350Z",
     "shell.execute_reply.started": "2024-01-17T21:16:49.423637Z",
     "shell.execute_reply": "2024-01-17T21:16:49.615531Z"
    },
    "trusted": true
   },
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], max_length=64, truncation=True, padding=\"max_length\")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-17T21:16:51.040413Z",
     "iopub.execute_input": "2024-01-17T21:16:51.041111Z",
     "iopub.status.idle": "2024-01-17T21:16:51.045497Z",
     "shell.execute_reply.started": "2024-01-17T21:16:51.041076Z",
     "shell.execute_reply": "2024-01-17T21:16:51.044531Z"
    },
    "trusted": true
   },
   "execution_count": 32,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenized_datasets = datasets.map(tokenize_function, batched=True, num_proc=2, remove_columns=[\"text\"])\n",
    "tokenized_datasets"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-17T21:16:52.213450Z",
     "iopub.execute_input": "2024-01-17T21:16:52.213824Z",
     "iopub.status.idle": "2024-01-17T21:16:52.682304Z",
     "shell.execute_reply.started": "2024-01-17T21:16:52.213797Z",
     "shell.execute_reply": "2024-01-17T21:16:52.681165Z"
    },
    "trusted": true
   },
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "text": "    ",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "#0:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "5831057b1dbe4bb68aeb07ff8c277d97"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "#1:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "3bc05a70e4574dde924a3b68145ba788"
      }
     },
     "metadata": {}
    },
    {
     "name": "stdout",
     "text": "    ",
     "output_type": "stream"
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "#0:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "906952023c69400d82dbd8d9b2f47286"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "#1:   0%|          | 0/1 [00:00<?, ?ba/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "197d66156442486995facc8a6ecd5ed2"
      }
     },
     "metadata": {}
    },
    {
     "execution_count": 33,
     "output_type": "execute_result",
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 38\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask'],\n        num_rows: 38\n    })\n})"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def copy_input_ids(example):\n",
    "    example[\"labels\"] = example[\"input_ids\"].copy()\n",
    "    return example"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-17T21:16:54.890046Z",
     "iopub.execute_input": "2024-01-17T21:16:54.890823Z",
     "iopub.status.idle": "2024-01-17T21:16:54.895618Z",
     "shell.execute_reply.started": "2024-01-17T21:16:54.890783Z",
     "shell.execute_reply": "2024-01-17T21:16:54.894577Z"
    },
    "trusted": true
   },
   "execution_count": 34,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "tokenized_datasets = tokenized_datasets.map(copy_input_ids)\n",
    "tokenized_datasets"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-17T21:16:56.352250Z",
     "iopub.execute_input": "2024-01-17T21:16:56.352633Z",
     "iopub.status.idle": "2024-01-17T21:16:56.419912Z",
     "shell.execute_reply.started": "2024-01-17T21:16:56.352602Z",
     "shell.execute_reply": "2024-01-17T21:16:56.418986Z"
    },
    "trusted": true
   },
   "execution_count": 35,
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/38 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "011622aa64bf4de9b5cfc492a6313761"
      }
     },
     "metadata": {}
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/38 [00:00<?, ?ex/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7d077fe1431e4c8ca111c19701c42fa1"
      }
     },
     "metadata": {}
    },
    {
     "execution_count": 35,
     "output_type": "execute_result",
     "data": {
      "text/plain": "DatasetDict({\n    train: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 38\n    })\n    test: Dataset({\n        features: ['input_ids', 'attention_mask', 'labels'],\n        num_rows: 38\n    })\n})"
     },
     "metadata": {}
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## –°—É—Ä–≥–∞–ª—Ç —Ö–∏–π—Ö\n",
    "\n",
    "Forward pass for `GPT2LMHeadModel` class\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/gpt2/modeling_gpt2.py#L1051"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_NAME)"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-17T21:25:19.608748Z",
     "iopub.execute_input": "2024-01-17T21:25:19.609431Z",
     "iopub.status.idle": "2024-01-17T21:25:21.866025Z",
     "shell.execute_reply.started": "2024-01-17T21:25:19.609394Z",
     "shell.execute_reply": "2024-01-17T21:25:21.864986Z"
    },
    "trusted": true
   },
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "### LoRA-–≥ –æ–¥–æ–æ apply —Ö–∏–π–Ω—ç.\n",
    "\n",
    "References:\n",
    "- https://arxiv.org/abs/2106.09685\n",
    "- https://pytorch.org/docs/stable/generated/torch.numel.html"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from peft import get_peft_config, PeftModel, PeftConfig, get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,  # task_type, token classification (TaskType.CAUSAL_LM)\n",
    "    inference_mode=False,\n",
    "    r=8,                           # r, the dimension of the low-rank matrices\n",
    "    lora_alpha=16,                 # lora_alpha, scaling factor for the weight matrices\n",
    "    lora_dropout=0.3,              # lora_dropout, dropout probability of the LoRA layers\n",
    "    fan_in_fan_out=True,\n",
    "    bias=\"lora_only\"               # bias, set to only lora layers to train\n",
    "    \n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-17T21:25:27.284153Z",
     "iopub.execute_input": "2024-01-17T21:25:27.285253Z",
     "iopub.status.idle": "2024-01-17T21:25:27.290415Z",
     "shell.execute_reply.started": "2024-01-17T21:25:27.285207Z",
     "shell.execute_reply": "2024-01-17T21:25:27.289529Z"
    },
    "trusted": true
   },
   "execution_count": 37,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "lora_model = get_peft_model(model, peft_config)\n",
    "lora_model.print_trainable_parameters()"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-17T21:25:30.599402Z",
     "iopub.execute_input": "2024-01-17T21:25:30.600250Z",
     "iopub.status.idle": "2024-01-17T21:25:30.633258Z",
     "shell.execute_reply.started": "2024-01-17T21:25:30.600208Z",
     "shell.execute_reply": "2024-01-17T21:25:30.632260Z"
    },
    "trusted": true
   },
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "text": "trainable params: 294,912 || all params: 124,734,720 || trainable%: 0.23643136409814364\n",
     "output_type": "stream"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "# https://huggingface.co/docs/transformers/main_classes/trainer#transformers.TrainingArguments\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    \"lora-mn-gpt2-on-dlub\",\n",
    "    \n",
    "    num_train_epochs=400,\n",
    "    per_device_train_batch_size=32,\n",
    "    per_device_eval_batch_size=32,\n",
    "    dataloader_num_workers=2,\n",
    "\n",
    "    evaluation_strategy = \"steps\",\n",
    "    logging_strategy=\"steps\",\n",
    "    save_strategy=\"steps\",\n",
    "    eval_steps=30,\n",
    "    logging_steps=30,\n",
    "    save_steps=30,\n",
    "\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=0.01,\n",
    "    save_total_limit=10,\n",
    "    report_to='none',\n",
    "\n",
    "    load_best_model_at_end=True,\n",
    "\n",
    "    # automatic version handling with huggingface\n",
    "    # push_to_hub=True,\n",
    ")"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-17T21:25:32.028856Z",
     "iopub.execute_input": "2024-01-17T21:25:32.029707Z",
     "iopub.status.idle": "2024-01-17T21:25:32.039027Z",
     "shell.execute_reply.started": "2024-01-17T21:25:32.029672Z",
     "shell.execute_reply": "2024-01-17T21:25:32.038062Z"
    },
    "trusted": true
   },
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from datasets import load_metric\n"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-17T21:25:34.313367Z",
     "iopub.execute_input": "2024-01-17T21:25:34.313768Z",
     "iopub.status.idle": "2024-01-17T21:25:34.318590Z",
     "shell.execute_reply.started": "2024-01-17T21:25:34.313736Z",
     "shell.execute_reply": "2024-01-17T21:25:34.317457Z"
    },
    "trusted": true
   },
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "def compute_metrics(eval_preds):\n",
    "    predictions, labels = eval_preds\n",
    "    primt(predictions, labels)\n",
    "    if isinstance(predictions, tuple):\n",
    "        predictions = predictions[0]\n",
    "    decoded_preds = tokenizer.batch_decode(predictions, skip_special_tokens=True)\n",
    "    decoded_labels = [[tokenizer.decode(l, skip_special_tokens=True)] for l in labels]\n",
    "    bleu_result = bleu_metric.compute(predictions=decoded_preds, references=decoded_labels)\n",
    "    return {\"bleu\": bleu_result[\"score\"]}"
   ],
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-17T21:25:35.621380Z",
     "iopub.execute_input": "2024-01-17T21:25:35.622151Z",
     "iopub.status.idle": "2024-01-17T21:25:35.628167Z",
     "shell.execute_reply.started": "2024-01-17T21:25:35.622119Z",
     "shell.execute_reply": "2024-01-17T21:25:35.627232Z"
    },
    "trusted": true
   },
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    "    compute_metrics=compute_metrics\n",
    ")"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "train_output = trainer.train()\n",
    "print(train_output)\n",
    "print(\"BLEU scores –ø–æ —ç–ø–æ—Ö–∞–º:\", bleu_scores)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "trainer.model.save_pretrained(\"lora-mn-gpt2-on-dlub\")"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## `generate` —Ö–∏–π—Ö"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "import torch"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# prompt = \"DLUB-–∏–π–Ω —Ö—ç–¥ –¥—ç—Ö —É–¥–∞–∞–≥–∏–π–Ω —Å—É—Ä–≥–∞–ª—Ç —è–≤–∞–≥–¥–∞—Ö –≥—ç–∂ –±–∞–π–≥–∞–∞ –≤—ç?\"  # in train data\n",
    "prompt = \"DLUB-–≥–∏–π–Ω –∑–æ—Ö–∏–æ–Ω –±–∞–π–≥—É—É–ª–∞–≥—á –Ø.–ë–∞—è—Ä—Ü–æ–≥—Ç –Ω—å —è–º–∞—Ä –±–∞–π–≥—É—É–ª–ª–∞–≥–∞—Ç –∞–∂–∏–ª–ª–∞–¥–∞–≥ –≤—ç?\"  # NOT in train data - but similar\n",
    "encoded_prompt = tokenizer(prompt, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "encoded_prompt = encoded_prompt.to(trainer.model.device)\n",
    "\n",
    "# prediction\n",
    "output_sequences = trainer.model.generate(\n",
    "    input_ids=encoded_prompt,\n",
    "    max_length=64,\n",
    "    min_length=1,\n",
    "    temperature=1.,\n",
    "    top_p=0.95,\n",
    "    do_sample=True,\n",
    "    num_return_sequences=10,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    ")\n",
    "\n",
    "generated_sequences = []\n",
    "\n",
    "# decode prediction\n",
    "for generated_sequence_idx, generated_sequence in enumerate(output_sequences):\n",
    "    generated_sequence = generated_sequence.tolist()\n",
    "    text = tokenizer.decode(generated_sequence, clean_up_tokenization_spaces=True, skip_special_tokens=False)\n",
    "    generated_sequences.append(text.strip())"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "corpus_bleu –¥–ª—è —Ç–µ–∫—Å—Ç–æ–≤"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "generated_sequences"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "\n",
    "reference = [['—ç—Ç–æ', '—ç—Ç–∞–ª–æ–Ω–Ω—ã–π', '—Ç–µ–∫—Å—Ç', '–Ω–∞', '—Ä—É—Å—Å–∫–æ–º']]\n",
    "candidate = ['—ç—Ç–æ', '—Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π', '—Ç–µ–∫—Å—Ç', '–Ω–∞', '—Ä—É—Å—Å–∫–æ–º']\n",
    "\n",
    "bleu_score = sentence_bleu(reference, candidate)\n",
    "print(f\"BLEU Score: {bleu_score}\")"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "bleu - —Å—Ö–æ–∂–µ—Å—Ç—å –º–µ–∂–¥—É —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º –∏ —ç—Ç–∞–ª–æ–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º, –∏—Å–ø–æ–ª—å–∑—É—è —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ n-–≥—Ä–∞–º–º (–ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω—ã—Ö –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç–µ–π –∏–∑ n —ç–ª–µ–º–µ–Ω—Ç–æ–≤)\n",
    "\n",
    "1) N-gram Precision:\n",
    "–í—ã—á–∏—Å–ª—è–µ—Ç—Å—è —Ç–æ—á–Ω–æ—Å—Ç—å –∫–∞–∂–¥–æ–≥–æ –ø–æ—Ä—è–¥–∫–∞ n-–≥—Ä–∞–º–º (unigram, bigram, trigram –∏ —Ç–∞–∫ –¥–∞–ª–µ–µ) –¥–ª—è —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞ –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —ç—Ç–∞–ª–æ–Ω–Ω—ã–º —Ç–µ–∫—Å—Ç–æ–º. \n",
    "\n",
    "2) Brevity Penalty (–®—Ç—Ä–∞—Ñ –∑–∞ –∫—Ä–∞—Ç–∫–æ—Å—Ç—å):\n",
    "–ü—Ä–∏–º–µ–Ω—è–µ—Ç—Å—è —à—Ç—Ä–∞—Ñ –∑–∞ –∫—Ä–∞—Ç–∫–æ—Å—Ç—å –¥–ª—è —É—á–µ—Ç–∞ —Ç–æ–≥–æ, –Ω–∞—Å–∫–æ–ª—å–∫–æ –∫–æ—Ä–æ—Ç–∫–∏–º –º–æ–∂–µ—Ç –±—ã—Ç—å —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –ø–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—é —Å —ç—Ç–∞–ª–æ–Ω–æ–º.\n",
    "\n",
    "3) BLEU Score:\n",
    "–ò—Ç–æ–≥–æ–≤—ã–π BLEU Score —Ä–∞—Å—Å—á–∏—Ç—ã–≤–∞–µ—Ç—Å—è –∫–∞–∫ –≥–µ–æ–º–µ—Ç—Ä–∏—á–µ—Å–∫–æ–µ —Å—Ä–µ–¥–Ω–µ–µ —Ç–æ—á–Ω–æ—Å—Ç–∏ n-–≥—Ä–∞–º–º —Å –≤–∑–≤–µ—à–∏–≤–∞–Ω–∏–µ–º —à—Ç—Ä–∞—Ñ–∞ –∑–∞ –∫—Ä–∞—Ç–∫–æ—Å—Ç—å."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "reference_bleu = [tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True, skip_special_tokens=False).split()]\n",
    "reference_bleu"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "for idx, generated_text in enumerate(generated_sequences):\n",
    "    candidate = generated_text.split()\n",
    "    bleu_score = sentence_bleu(reference_bleu, candidate)\n",
    "    print(f\"BLEU Score {idx + 1}: {bleu_score}\")\n",
    "\n",
    "average_bleu_score = sum([sentence_bleu(reference_bleu, generated_text.split()) for generated_text in generated_sequences]) / len(generated_sequences)\n",
    "print(f\"Average BLEU Score: {average_bleu_score}\")"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "!pip install rouge"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from rouge import Rouge"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "references_rouge = tokenizer.decode(encoded_prompt[0], clean_up_tokenization_spaces=True, skip_special_tokens=False).strip()\n",
    "references_rouge"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "rouge = Rouge()\n",
    "\n",
    "for idx, generated_text in enumerate(generated_sequences):\n",
    "    rouge_scores = rouge.get_scores(generated_text, references_rouge)\n",
    "    print(f\"ROUGE Scores {idx + 1}: {rouge_scores}\")\n",
    "\n",
    "average_rouge_1 = sum([score[\"rouge-1\"][\"f\"] for score in rouge_scores]) / len(rouge_scores)\n",
    "print(f\"Average ROUGE-1: {average_rouge_1}\")\n"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "generated_sequences[0]"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "ROUGE –º–µ—Ç—Ä–∏–∫–∏ –æ—Ä–∏–µ–Ω—Ç–∏—Ä–æ–≤–∞–Ω—ã –Ω–∞ –æ—Ü–µ–Ω–∫—É —Å—Ö–æ–∂–µ—Å—Ç–∏ –º–µ–∂–¥—É –¥–≤—É–º—è —Ç–µ–∫—Å—Ç–∞–º–∏, –ø–æ—ç—Ç–æ–º—É —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ –≤—ã–ø–æ–ª–Ω—è–µ—Ç—Å—è –Ω–∞ —É—Ä–æ–≤–Ω–µ –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏–π –∏–ª–∏ —Å–ª–æ–≤ –≤ –∑–∞–≤–∏—Å–∏–º–æ—Å—Ç–∏ –æ—Ç –≤—ã–±—Ä–∞–Ω–Ω–æ–π –º–µ—Ç—Ä–∏–∫–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, rouge-1 —Å—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç —É–Ω–∏–≥—Ä–∞–º–º—ã).\n",
    "–æ—Ü–µ–Ω–∫–∞ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å —É–Ω–∏–≥—Ä–∞–º–º –≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –æ—Ç–≤–µ—Ç–µ —Å —ç—Ç–∞–ª–æ–Ω–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º.\n",
    "ROUGE-1 F1-–æ—Ü–µ–Ω–∫–∞ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç —Å—Ö–æ–∂–µ—Å—Ç—å —É–Ω–∏–≥—Ä–∞–º–º –≤ —Å–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞–Ω–Ω–æ–º –æ—Ç–≤–µ—Ç–µ —Å —ç—Ç–∞–ª–æ–Ω–Ω—ã–º –æ—Ç–≤–µ—Ç–æ–º.\n",
    "\n",
    "ROUGE –º–µ—Ç—Ä–∏–∫–∏ –≤–∫–ª—é—á–∞—é—Ç –≤ —Å–µ–±—è, –Ω–∞–ø—Ä–∏–º–µ—Ä:\n",
    "\n",
    "rouge-1: —É–Ω–∏–≥—Ä–∞–º–º—ã\n",
    "rouge-2: –±–∏–≥—Ä–∞–º–º—ã\n",
    "rouge-l: –Ω–∞–∏–±–æ–ª—å—à–∞—è –æ–±—â–∞—è –ø–æ—Å–ª–µ–¥–æ–≤–∞—Ç–µ–ª—å–Ω–æ—Å—Ç—å"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "import torch"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "input_ids = tokenizer(generated_text, return_tensors=\"pt\").input_ids\n",
    "    with torch.no_grad():\n",
    "        logits = model(input_ids).logits\n",
    "        target_ids = input_ids[:, 1:].contiguous()  \n",
    "        loss = torch.nn.functional.cross_entropy(logits[:, :-1, :].reshape(-1, logits.shape[-1]), target_ids.view(-1))\n",
    "\n",
    "    perplexity = torch.exp(loss)"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## –î–∞—Ä–∞–∞ –Ω—å —Ö—ç—Ä—Ö—ç–Ω —É–Ω—à–∏—Ö –≤—ç?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": [
    "prompt_in_train = \"DLUB-–≥–∏–π–Ω –∑–æ—Ö–∏–æ–Ω –±–∞–π–≥—É—É–ª–∞–≥—á –Ø.–ë–∞—è—Ä—Ü–æ–≥—Ç –Ω—å —Ö–∞–∞–Ω–∞ –∞–∂–∏–ª–ª–∞–¥–∞–≥ –≤—ç?\"  # in train data\n",
    "prompt_not_in_train = \"DLUB-–≥–∏–π–Ω –∑–æ—Ö–∏–æ–Ω –±–∞–π–≥—É—É–ª–∞–≥—á –Ø.–ë–∞—è—Ä—Ü–æ–≥—Ç –Ω—å —è–º–∞—Ä –±–∞–π–≥—É—É–ª–ª–∞–≥–∞—Ç –∞–∂–∏–ª–ª–∞–¥–∞–≥ –≤—ç?\"  # NOT in train data - but similar\n",
    "encoded_prompt_in_train = tokenizer(prompt_in_train, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "encoded_prompt_not_in_train = tokenizer(prompt_not_in_train, add_special_tokens=False, return_tensors=\"pt\").input_ids\n",
    "\n",
    "_model = PeftModel.from_pretrained(model, \"lora-mn-gpt2-on-dlub\").to(\"cpu\")\n",
    "\n",
    "for _encoded_prompt in [encoded_prompt_in_train, encoded_prompt_not_in_train]:\n",
    "    output_sequences = _model.generate(\n",
    "        input_ids=_encoded_prompt,\n",
    "        max_length=64,\n",
    "        min_length=10,\n",
    "        temperature=1.,\n",
    "        top_p=0.95,\n",
    "        do_sample=True,\n",
    "        num_return_sequences=1,\n",
    "        pad_token_id=tokenizer.pad_token_id,\n",
    "    )\n",
    "\n",
    "    text = tokenizer.decode(output_sequences[0], clean_up_tokenization_spaces=True, skip_special_tokens=False)\n",
    "\n",
    "    # –î—ç–º–æ-–¥ –∑–æ—Ä–∏—É–ª–∂ —Ö—è–ª–±–∞—Ä—á–∏–ª—ä—è:\n",
    "    question, answer = text.split(\"?\")[:2]\n",
    "    answer = answer.split(\".\")[0]\n",
    "    print(question + \"?\", answer )"
   ],
   "metadata": {
    "trusted": true
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
