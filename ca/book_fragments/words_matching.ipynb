{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: sentence-transformers in f:\\grammar_ru\\.conda\\lib\\site-packages (3.0.0)\n",
      "Requirement already satisfied: googletrans==3.1.0a0 in f:\\grammar_ru\\.conda\\lib\\site-packages (3.1.0a0)\n",
      "Requirement already satisfied: httpx==0.13.3 in f:\\grammar_ru\\.conda\\lib\\site-packages (from googletrans==3.1.0a0) (0.13.3)\n",
      "Requirement already satisfied: certifi in f:\\grammar_ru\\.conda\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2023.7.22)\n",
      "Requirement already satisfied: hstspreload in f:\\grammar_ru\\.conda\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2023.1.1)\n",
      "Requirement already satisfied: sniffio in f:\\grammar_ru\\.conda\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.3.0)\n",
      "Requirement already satisfied: chardet==3.* in f:\\grammar_ru\\.conda\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (3.0.4)\n",
      "Requirement already satisfied: idna==2.* in f:\\grammar_ru\\.conda\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (2.10)\n",
      "Requirement already satisfied: rfc3986<2,>=1.3 in f:\\grammar_ru\\.conda\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (1.5.0)\n",
      "Requirement already satisfied: httpcore==0.9.* in f:\\grammar_ru\\.conda\\lib\\site-packages (from httpx==0.13.3->googletrans==3.1.0a0) (0.9.1)\n",
      "Requirement already satisfied: h11<0.10,>=0.8 in f:\\grammar_ru\\.conda\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (0.9.0)\n",
      "Requirement already satisfied: h2==3.* in f:\\grammar_ru\\.conda\\lib\\site-packages (from httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.2.0)\n",
      "Requirement already satisfied: hyperframe<6,>=5.2.0 in f:\\grammar_ru\\.conda\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (5.2.0)\n",
      "Requirement already satisfied: hpack<4,>=3.0 in f:\\grammar_ru\\.conda\\lib\\site-packages (from h2==3.*->httpcore==0.9.*->httpx==0.13.3->googletrans==3.1.0a0) (3.0.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.34.0 in f:\\grammar_ru\\.conda\\lib\\site-packages (from sentence-transformers) (4.40.1)\n",
      "Requirement already satisfied: tqdm in f:\\grammar_ru\\.conda\\lib\\site-packages (from sentence-transformers) (4.64.1)\n",
      "Requirement already satisfied: torch>=1.11.0 in f:\\grammar_ru\\.conda\\lib\\site-packages (from sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: numpy in f:\\grammar_ru\\.conda\\lib\\site-packages (from sentence-transformers) (1.24.1)\n",
      "Requirement already satisfied: scikit-learn in f:\\grammar_ru\\.conda\\lib\\site-packages (from sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: scipy in f:\\grammar_ru\\.conda\\lib\\site-packages (from sentence-transformers) (1.10.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.15.1 in f:\\grammar_ru\\.conda\\lib\\site-packages (from sentence-transformers) (0.22.2)\n",
      "Requirement already satisfied: Pillow in f:\\grammar_ru\\.conda\\lib\\site-packages (from sentence-transformers) (9.4.0)\n",
      "Requirement already satisfied: filelock in f:\\grammar_ru\\.conda\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (3.8.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in f:\\grammar_ru\\.conda\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in f:\\grammar_ru\\.conda\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (23.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in f:\\grammar_ru\\.conda\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (6.0)\n",
      "Requirement already satisfied: requests in f:\\grammar_ru\\.conda\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (2.28.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in f:\\grammar_ru\\.conda\\lib\\site-packages (from huggingface-hub>=0.15.1->sentence-transformers) (4.4.0)\n",
      "Requirement already satisfied: colorama in f:\\grammar_ru\\.conda\\lib\\site-packages (from tqdm->sentence-transformers) (0.4.6)\n",
      "Requirement already satisfied: regex!=2019.12.17 in f:\\grammar_ru\\.conda\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (2022.10.31)\n",
      "Requirement already satisfied: tokenizers<0.20,>=0.19 in f:\\grammar_ru\\.conda\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.19.1)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in f:\\grammar_ru\\.conda\\lib\\site-packages (from transformers<5.0.0,>=4.34.0->sentence-transformers) (0.4.3)\n",
      "Requirement already satisfied: joblib>=1.1.1 in f:\\grammar_ru\\.conda\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.2.0)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in f:\\grammar_ru\\.conda\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.1.0)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in f:\\grammar_ru\\.conda\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (2.1.1)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in f:\\grammar_ru\\.conda\\lib\\site-packages (from requests->huggingface-hub>=0.15.1->sentence-transformers) (1.26.14)\n"
     ]
    }
   ],
   "source": [
    "!pip install sentence-transformers googletrans==3.1.0a0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grammar_ru.corpus import ParallelCorpus\n",
    "from utils.make_match import make_match\n",
    "from ca import Seq2VecMatcher\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "parallel_corpus = ParallelCorpus(Path('./files/parallel_corpus.zip'))\n",
    "en_book_relations = parallel_corpus.en_book.get_relations()\n",
    "en_book_relations = en_book_relations[en_book_relations['relation_name'] == 'en_book_en_retell']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "finished_tasks_num = 0\n",
    "err_relations = []\n",
    "matcher = Seq2VecMatcher()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['95563950-9e63-46a0-99df-e6c5b2e4528a',\n",
       " '4be61168-5f93-48c5-b96e-dd4346a8f2c4',\n",
       " '0f96c70e-638b-414c-abe7-117595c52b56']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_book_relations.iloc[1:4, 0].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = f\"data/result_dataset.json\"\n",
    "os.makedirs(os.path.dirname(file_path), exist_ok=True)\n",
    "\n",
    "with open(file_path, 'a', encoding='utf-8') as file:\n",
    "    file.write('[')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(en_book_relations)):\n",
    "    print(f\"working on {i + 1}/{len(en_book_relations)}\", end='\\r')\n",
    "\n",
    "    book_id = en_book_relations.iloc[i, 0]\n",
    "    retell_id = en_book_relations.iloc[i, 1]\n",
    "\n",
    "    book_chapter_df = parallel_corpus.en_book.get_frames(uids=[book_id]).first()\n",
    "    retell_chapter_df = parallel_corpus.en_retell.get_frames(uids=[retell_id]).first()\n",
    "    \n",
    "    try:\n",
    "        sent_dataset_json = make_match(book_chapter_df, retell_chapter_df, matcher)\n",
    "\n",
    "        with open(file_path, 'a', encoding='utf-8') as file:\n",
    "            file.write(sent_dataset_json[1:-1])\n",
    "            if i != len(en_book_relations) - 1:\n",
    "                file.write(',')\n",
    "        \n",
    "        print(\"                                                                    \", end='\\r')\n",
    "    except Exception as ex:\n",
    "        err_relations.append(((book_id, retell_id), ex))\n",
    "\n",
    "with open(file_path, 'a', encoding='utf-8') as file:\n",
    "    file.write(']')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "index 10 is out of bounds for axis 0 with size 10",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[31], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m err_relations[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m1\u001b[39m]\n",
      "Cell \u001b[1;32mIn[20], line 12\u001b[0m\n\u001b[0;32m      8\u001b[0m retell_chapter_df \u001b[38;5;241m=\u001b[39m parallel_corpus\u001b[38;5;241m.\u001b[39men_retell\u001b[38;5;241m.\u001b[39mget_frames(uids\u001b[38;5;241m=\u001b[39m[retell_id])\u001b[38;5;241m.\u001b[39mfirst()\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 12\u001b[0m     matched_ids, matched_sentences \u001b[38;5;241m=\u001b[39m \u001b[43mmatcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_matches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbook_chapter_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretell_chapter_df\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneed_matching_df\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m     matched_ids[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMatchedWithSentence\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries(matched_sentences\u001b[38;5;241m.\u001b[39marray)\n\u001b[0;32m     14\u001b[0m     matched_ids \u001b[38;5;241m=\u001b[39m matched_ids\u001b[38;5;241m.\u001b[39mdropna()\n",
      "File \u001b[1;32mf:\\grammar_ru\\ca\\Seq2VecMatching.py:21\u001b[0m, in \u001b[0;36mSeq2VecMatcher.get_matches\u001b[1;34m(self, df_1, df_2, need_matching_df)\u001b[0m\n\u001b[0;32m     19\u001b[0m action_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_action_matrix(cos_sim)\n\u001b[0;32m     20\u001b[0m monotone_matching \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_monotone_matching(action_matrix)\n\u001b[1;32m---> 21\u001b[0m matched_sent \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries({text_1[t_1_id]: text_2[t_2_id] \u001b[38;5;28;01mfor\u001b[39;00m t_1_id, t_2_id \u001b[38;5;129;01min\u001b[39;00m monotone_matching\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[0;32m     22\u001b[0m                          name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatched_sentence\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m need_matching_df:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m matched_sent\n",
      "File \u001b[1;32mf:\\grammar_ru\\ca\\Seq2VecMatching.py:21\u001b[0m, in \u001b[0;36m<dictcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     19\u001b[0m action_matrix \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_action_matrix(cos_sim)\n\u001b[0;32m     20\u001b[0m monotone_matching \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_monotone_matching(action_matrix)\n\u001b[1;32m---> 21\u001b[0m matched_sent \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries({\u001b[43mtext_1\u001b[49m\u001b[43m[\u001b[49m\u001b[43mt_1_id\u001b[49m\u001b[43m]\u001b[49m: text_2[t_2_id] \u001b[38;5;28;01mfor\u001b[39;00m t_1_id, t_2_id \u001b[38;5;129;01min\u001b[39;00m monotone_matching\u001b[38;5;241m.\u001b[39mitems()},\n\u001b[0;32m     22\u001b[0m                          name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatched_sentence\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m need_matching_df:\n\u001b[0;32m     24\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m matched_sent\n",
      "\u001b[1;31mIndexError\u001b[0m: index 10 is out of bounds for axis 0 with size 10"
     ]
    }
   ],
   "source": [
    "raise err_relations[0][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "data = None\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    data = json.load(file)\n",
    "\n",
    "print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
